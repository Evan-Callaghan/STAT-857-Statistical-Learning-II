{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28c0e563-4057-438a-982c-1d08aabae39e",
   "metadata": {},
   "source": [
    "## STAT 857 - Project 1 Python Code\n",
    "## Evan Callaghan | March 27, 2023\n",
    "\n",
    "#### File Overview:\n",
    "- Installing necessary libraries\n",
    "- Imporing necessay libraries\n",
    "- Exploratory data analysis\n",
    "    - Producing table and plots of the data\n",
    "- Data cleaning and feature engineering \n",
    "- Hyper-parameter tuning\n",
    "- Model Testing\n",
    "    - Testing a variety of model types with different subsets of input features\n",
    "- Final modelling\n",
    "    - Using the best models from model testing\n",
    "- Ensemble methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0654c40-e32c-4d56-8f84-901e1fed63c0",
   "metadata": {},
   "source": [
    "#### Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca49c92f-6aab-4fbe-8609-bbf4284bd61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install haversine xgboost lightgbm optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb4eaec-a346-4263-8f3d-9a0bb8a4d9f4",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7c268-f853-485e-a1d4-e1219e79083e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from haversine import haversine, Unit\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b99d174-9ac6-417c-b111-73fbfa753a8d",
   "metadata": {},
   "source": [
    "#### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdb6c7a-4ca5-49a0-8192-dfa6e025d60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "train = pd.read_csv('Data/W23P1_train_google.csv')\n",
    "test = pd.read_csv('Data/W23P1_test_google.csv')\n",
    "sub = pd.read_csv('Data/W23P1_sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cccbe5a-5ede-44c2-9e97-0bee0adbb329",
   "metadata": {},
   "source": [
    "Note: these are the outputted functions from the file W23P1_r.R (i.e., they include the engineered variables from the Google API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8b4744-2c9c-4013-b075-2944ac37f0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f130d-c115-4223-b813-dadf6f69cd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874d3d2-34f9-4c23-af9c-ad251ca8845d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['pickup_datetime'] = pd.to_datetime(train['pickup_datetime'], format = '%Y-%m-%d %H:%M:%S UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25635198-da72-417d-bbae-5e378ec3eac0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.describe(datetime_is_numeric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f76540-430c-4bb7-bc24-fd7453459730",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (10, 4))\n",
    "ax.hist('fare_amount', data = train, bins = 50, edgecolor = 'black')\n",
    "plt.title('Histogram of Fare Amount')\n",
    "plt.xlabel('Fare Amount ($)')\n",
    "plt.xticks(np.arange(0, 170, 10))\n",
    "plt.ylabel('Count')\n",
    "ax.grid(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce2de51-1970-42e0-a65d-c99db6f016dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 4))\n",
    "plt.plot('distance', 'fare_amount', data = train, linestyle = 'none', marker = 'o', markersize = 3)\n",
    "plt.title('Training Data Ride Distance vs. Fare Amount')\n",
    "plt.xlabel('Ride Distance (miles)')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631b62f6-4aca-4b9e-874c-27ae272d072a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 4))\n",
    "plt.plot('duration', 'fare_amount', data = train, linestyle = 'none', marker = 'o', markersize = 3)\n",
    "plt.title('Training Data Duration vs. Fare Amount')\n",
    "plt.xlabel('Ride Duration (minutes)')\n",
    "plt.ylabel('Fare Amount ($)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4145e-8486-4aeb-a0ec-02528e1dcb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = train.groupby('passenger_count')['fare_amount'].mean()\n",
    "\n",
    "plt.figure(figsize = (8, 4))\n",
    "plt.bar(data.index, data.values)\n",
    "plt.title('Average Fare Amount by Count of Ride Passengers')\n",
    "plt.xticks(data.index)\n",
    "plt.xlabel('Number of passengers')\n",
    "plt.ylabel('Average Fare Amount ($)')\n",
    "plt.grid(alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc620e90-04b5-4464-989e-98d629fc0014",
   "metadata": {},
   "source": [
    "Note: the visualizations produced for the final report were created using Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c1107-0c96-450c-8829-a3602c9f139c",
   "metadata": {},
   "source": [
    "#### Data Cleaning and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe64fff-6804-4e18-9eb6-75a4adbffbff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining feature engineering function\n",
    "def feature_engineering(df):\n",
    "    \n",
    "    ## Location variables\n",
    "    df['haversine'] = np.nan\n",
    "    for i in range(0, df.shape[0]):\n",
    "        pickup = (df.at[i, 'pickup_latitude'], df.at[i, 'pickup_longitude'])\n",
    "        dropoff = (df.at[i, 'dropoff_latitude'], df.at[i, 'dropoff_longitude'])\n",
    "        df.at[i, 'haversine'] = haversine(pickup, dropoff, unit = 'mi')\n",
    "    df['same_lat'] = np.where(df['pickup_latitude'] == df['dropoff_latitude'], 1, 0)\n",
    "    df['same_long'] = np.where(df['pickup_longitude'] == df['dropoff_longitude'], 1, 0)\n",
    "    df['same_coord'] = np.where((df['same_lat'] == 1) & (df['same_long']), 1, 0)\n",
    "    df['pickup_longitude_rounded'] = np.round(df['pickup_longitude'], 2)\n",
    "    df['pickup_latitude_rounded'] = np.round(df['pickup_latitude'], 2)\n",
    "    df['dropoff_longitude_rounded'] = np.round(df['dropoff_longitude'], 2)\n",
    "    df['dropoff_latitude_rounded'] = np.round(df['dropoff_latitude'], 2)\n",
    "    df['same_lat_rounded'] = np.where(df['pickup_latitude_rounded'] == df['dropoff_latitude_rounded'], 1, 0)\n",
    "    df['same_long_rounded'] = np.where(df['pickup_longitude_rounded'] == df['dropoff_longitude_rounded'], 1, 0)\n",
    "    df['same_coord_rounded'] = np.where((df['same_lat_rounded'] == 1) & (df['same_long_rounded']), 1, 0)\n",
    "\n",
    "    ## Date/time variables \n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format = '%Y-%m-%d %H:%M:%S UTC')\n",
    "    df['pickup_date'] = df['pickup_datetime'].dt.date\n",
    "    df['pickup_day'] = df['pickup_datetime'].apply(lambda x:x.day)\n",
    "    df['pickup_hour'] = df['pickup_datetime'].apply(lambda x:x.hour)\n",
    "    df['pickup_day_of_week'] = df['pickup_datetime'].apply(lambda x:calendar.day_name[x.weekday()])\n",
    "    df = pd.concat([df, pd.get_dummies(pd.Series(df['pickup_day_of_week']))], axis = 1)\n",
    "    df['weekend'] = np.where((df['Saturday'] == 1) | (df['Sunday'] == 1), 1, 0)\n",
    "    df['morning'] = np.where((df['pickup_hour'] >= 3) & (df['pickup_hour'] < 7), 1, 0)\n",
    "    df['rush_hour'] = np.where((df['pickup_hour'] >= 16) & (df['pickup_hour'] < 20) & (df['weekend'] == 0), 1, 0)\n",
    "    df['overnight'] = np.where((df['pickup_hour'] < 6) | (df['pickup_hour'] > 20), 1, 0)\n",
    "    df['holiday'] = np.where((df['pickup_day'] == 1) | (df['pickup_day'] == 21), 1, 0)\n",
    "\n",
    "    ## Airport variables \n",
    "    df['pickup_LGA'] = df.apply(lambda row:isAirport(row['pickup_latitude'], row['pickup_longitude'], 'LGA'), axis = 1)\n",
    "    df['dropoff_LGA'] = df.apply(lambda row:isAirport(row['dropoff_latitude'], row['dropoff_longitude'], 'LGA'), axis = 1)\n",
    "    df['LGA'] = np.where(df['pickup_LGA'] + df['dropoff_LGA'] > 0, 1, 0)\n",
    "    df['pickup_JFK'] = df.apply(lambda row:isAirport(row['pickup_latitude'], row['pickup_longitude'], 'JFK'), axis = 1)\n",
    "    df['dropoff_JFK'] = df.apply(lambda row:isAirport(row['dropoff_latitude'], row['dropoff_longitude'], 'JFK'), axis = 1)\n",
    "    df['JFK'] = np.where(df['pickup_JFK'] + df['dropoff_JFK'] > 0, 1, 0)\n",
    "    df['pickup_EWR'] = df.apply(lambda row:isAirport(row['pickup_latitude'], row['pickup_longitude'], 'EWR'), axis = 1)\n",
    "    df['dropoff_EWR'] = df.apply(lambda row:isAirport(row['dropoff_latitude'], row['dropoff_longitude'], 'EWR'), axis = 1)\n",
    "    df['EWR'] = np.where(df['pickup_EWR'] + df['dropoff_EWR'] > 0, 1, 0)\n",
    "    df['pickup_airport'] = np.where(df['pickup_LGA'] + df['pickup_JFK'] + df['pickup_EWR'] > 0, 1, 0)\n",
    "    df['dropoff_airport'] = np.where(df['dropoff_LGA'] + df['dropoff_JFK'] + df['dropoff_EWR'] > 0, 1, 0)\n",
    "    df['airport'] = np.where(df['LGA'] + df['JFK'] + df['EWR'] > 0, 1, 0)\n",
    "\n",
    "    ## Borough\n",
    "    df['pickup_borough'] = df.apply(lambda row:getBorough(row['pickup_latitude'], row['pickup_longitude']), axis = 1)\n",
    "    df['dropoff_borough'] = df.apply(lambda row:getBorough(row['dropoff_latitude'], row['dropoff_longitude']), axis = 1)\n",
    "    df['change_borough'] = np.where(df['pickup_borough'] != df['dropoff_borough'], 1, 0)\n",
    "    df = pd.concat([df, pd.get_dummies(pd.Series(df['pickup_borough']), prefix = 'pickup', prefix_sep = '_')], axis = 1)\n",
    "    df = pd.concat([df, pd.get_dummies(pd.Series(df['dropoff_borough']), prefix = 'dropoff', prefix_sep = '_')], axis = 1)\n",
    "    \n",
    "    return df;\n",
    "\n",
    "## Defining helper dictionaries\n",
    "nyc_airports = {'JFK':{'min_lng':-73.8352, 'min_lat':40.6195, 'max_lng':-73.7401, 'max_lat':40.6659}, \n",
    "                'EWR':{'min_lng':-74.1925, 'min_lat':40.6700,  'max_lng':-74.1531,  'max_lat':40.7081}, \n",
    "                'LGA':{'min_lng':-73.8895, 'min_lat':40.7664, 'max_lng':-73.8550, 'max_lat':40.7931}\n",
    "               }\n",
    "\n",
    "nyc_boroughs = {'manhattan':{'min_lng':-74.0479, 'min_lat':40.6829, 'max_lng':-73.9067, 'max_lat':40.8820},  \n",
    "                'brooklyn':{'min_lng':-74.0421, 'min_lat':40.5707, 'max_lng':-73.834, 'max_lat':40.7395}, \n",
    "                'queens':{'min_lng':-73.9630, 'min_lat':40.5431, 'max_lng':-73.7004, 'max_lat':40.8007},\n",
    "                'bronx':{'min_lng':-73.9339, 'min_lat':40.7855, 'max_lng':-73.7654, 'max_lat':40.9176}, \n",
    "                'staten_island':{'min_lng':-74.2558, 'min_lat':40.4960, 'max_lng':-74.0522, 'max_lat':40.6490}\n",
    "               }\n",
    "\n",
    "## Defining helper functions\n",
    "def isAirport(latitude, longitude, airport_name = 'JFK'):\n",
    "    \n",
    "    a = (latitude >= nyc_airports[airport_name]['min_lat'])\n",
    "    b = (latitude <= nyc_airports[airport_name]['max_lat'])\n",
    "    c = (longitude >= nyc_airports[airport_name]['min_lng'])\n",
    "    d = (longitude <= nyc_airports[airport_name]['max_lng'])\n",
    "    \n",
    "    if (a and b and c and d): return 1\n",
    "    else: return 0\n",
    "\n",
    "def getBorough(latitude, longitude):\n",
    "    \n",
    "    boroughs = nyc_boroughs.keys()\n",
    "    \n",
    "    for borough in boroughs:\n",
    "        \n",
    "        a = (latitude >= nyc_boroughs[borough]['min_lat'])\n",
    "        b = (latitude <= nyc_boroughs[borough]['max_lat'])\n",
    "        c = (longitude >= nyc_boroughs[borough]['min_lng'])\n",
    "        d = (longitude <= nyc_boroughs[borough]['max_lng'])\n",
    "        \n",
    "        if (a and b and c and d): return borough\n",
    "    \n",
    "    return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211bcac-9330-4944-8b6e-ebe06e8de044",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Feature engineering\n",
    "train = feature_engineering(train)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7813af14-b4c2-4447-873e-0ff372a15760",
   "metadata": {
    "tags": []
   },
   "source": [
    "More cleaning: a few obvious outliers that can be fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e7dc0-bc20-4785-9fbf-28b23f0b5bdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 2))\n",
    "plt.scatter('distance', 'haversine', data = train, marker = '.', s = 12)\n",
    "plt.title('Distance vs. Haversine (train)')\n",
    "plt.xlabel('Distance (Google)')\n",
    "plt.ylabel('Haversine')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6daf5d8-b4f3-444c-a103-c7cf356bfe82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 2))\n",
    "plt.scatter('distance', 'haversine', data = test, marker = '.', s = 12)\n",
    "plt.title('Distance vs. Haversine (test)')\n",
    "plt.xlabel('Distance (Google)')\n",
    "plt.ylabel('Haversine')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb0256-31b8-4bdb-bdb9-ef204bcced8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (4, 2))\n",
    "plt.scatter('duration', 'haversine', data = train, marker = '.', s = 12)\n",
    "plt.title('Duration vs. Haversine (train)')\n",
    "plt.xlabel('Duration (Google)')\n",
    "plt.ylabel('Haversine')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77672021-fd53-496d-8788-d3690d03518f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Cleaning the distance variable\n",
    "\n",
    "## Selecting observations which need updating\n",
    "train_index = train[(train['haversine'] < 2) & (train['distance'] > 9)].index\n",
    "test_index = test[(test['haversine'] < 2) & (test['distance'] > 9)].index\n",
    "\n",
    "## Subsetting the data\n",
    "to_train = train[np.isin(train.index, train_index, invert = True)]\n",
    "to_train_test = test[np.isin(test.index, test_index, invert = True)]\n",
    "\n",
    "to_fix = train[np.isin(train.index, train_index)]\n",
    "to_fix_test = test[np.isin(test.index, test_index)]\n",
    "\n",
    "## Defining input and target variables\n",
    "variables = ['haversine', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude', 'same_lat', 'same_long', \n",
    "             'same_coord', 'pickup_longitude_rounded', 'pickup_latitude_rounded', 'dropoff_longitude_rounded', 'dropoff_latitude_rounded', \n",
    "             'same_lat_rounded', 'same_long_rounded', 'same_coord_rounded']\n",
    "\n",
    "to_train_X = to_train[variables]\n",
    "to_train_Y = to_train['distance']\n",
    "\n",
    "to_train_X_test = to_train_test[variables]\n",
    "to_train_Y_test = to_train_test['distance']\n",
    "\n",
    "to_fix_X = to_fix[variables]\n",
    "to_fix_Y = to_fix['distance']\n",
    "\n",
    "to_fix_X_test = to_fix_test[variables]\n",
    "to_fix_Y_test = to_fix_test['distance']\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 3, n_estimators = 500).fit(to_train_X, to_train_Y)\n",
    "rf_md_test = RandomForestRegressor(max_depth = 3, n_estimators = 500).fit(to_train_X_test, to_train_Y_test)\n",
    "\n",
    "## Predicting on the fix set\n",
    "to_fix['distance'] = rf_md.predict(to_fix_X)\n",
    "to_fix_test['distance'] = rf_md_test.predict(to_fix_X_test)\n",
    "\n",
    "## Reconstructing the training and testing data-frames\n",
    "train = pd.concat([to_train, to_fix], axis = 0).sort_index()\n",
    "test = pd.concat([to_train_test, to_fix_test], axis = 0).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed4c26b-905c-4150-aeb1-b8b2658c4dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Cleaning the duration variable\n",
    "train_index = train[(train['haversine'] > 20) & (train['duration'] < 5)].index\n",
    "train = train[np.isin(train.index, train_index, invert = True)]\n",
    "\n",
    "## More cleaning for distance\n",
    "train['distance'] = np.where(train['haversine'] < 0.1, 0, train['distance'])\n",
    "test['distance'] = np.where(test['haversine'] < 0.1, 0, test['distance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1371e97-c0f8-4bf5-b10f-67857146f8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Creating time estimate variable\n",
    "X_time = train[['distance', 'duration', 'pickup_hour', 'weekend', 'morning', 'rush_hour', 'overnight', 'holiday']]\n",
    "Y_time = train['fare_amount']\n",
    "\n",
    "X_time_test = test[['distance', 'duration', 'pickup_hour', 'weekend', 'morning', 'rush_hour', 'overnight', 'holiday']]\n",
    "\n",
    "rf_md = RandomForestRegressor(max_depth = 10, n_estimators = 1000).fit(X_time, Y_time)\n",
    "train['time_estimate'] = rf_md.predict(X_time)\n",
    "test['time_estimate'] = rf_md.predict(X_time_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3fdb2-308d-4fee-8389-dd952c093d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98d5f8a-202b-4c1d-866c-cc38009f8178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b00864-c438-4d60-8cb7-0428d7497116",
   "metadata": {},
   "source": [
    "#### Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c628ef-20ce-4dff-b85c-4cf34eedaff3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining Optuna objective functions\n",
    "def rf_reg_objective(trial):\n",
    "\n",
    "    ## Defining the XGBoost hyper-parameter grid\n",
    "    rf_param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),\n",
    "                     'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                     'min_samples_split': trial.suggest_int('min_samples_split', 2, 20), \n",
    "                     'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20), \n",
    "                     'random_state': trial.suggest_int('random_state', 1, 500),\n",
    "                     'max_features': trial.suggest_categorical('max_features', ['sqrt', None])\n",
    "                    }\n",
    "    \n",
    "    ## Building the model\n",
    "    rf_md = RandomForestRegressor(**rf_param_grid, n_jobs = -1, criterion = 'squared_error').fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test data-frame\n",
    "    rf_md_preds = rf_md.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    rf_md_mse = mean_squared_error(Y_validation, rf_md_preds, squared = False)\n",
    "    \n",
    "    return rf_md_mse\n",
    "\n",
    "def hist_reg_objective(trial):\n",
    "\n",
    "    ## Defining the XGBoost hyper-parameter grid\n",
    "    hist_param_grid = {'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                       'max_iter': trial.suggest_int('n_estimators', 100, 1000, 50),\n",
    "                       'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                       'l2_regularization': trial.suggest_float('l2_regularization', 0, 0.1, step = 0.002),\n",
    "                       'random_state': trial.suggest_int('random_state', 1, 500),\n",
    "                      }\n",
    "    \n",
    "    ## Building the model\n",
    "    hist_md = HistGradientBoostingRegressor(**hist_param_grid, loss = 'squared_error', early_stopping = True).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test data-frame\n",
    "    hist_md_preds = hist_md.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    hist_md_mse = mean_squared_error(Y_validation, hist_md_preds, squared = False)\n",
    "    \n",
    "    return hist_md_mse\n",
    "\n",
    "def xgb_reg_objective(trial):\n",
    "\n",
    "    ## Defining the XGBoost hyper-parameter grid\n",
    "    xgboost_param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50), \n",
    "                          'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                          'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                          'gamma': trial.suggest_float('gamma', 0, 0.3, step = 0.05), \n",
    "                          'min_child_weight': trial.suggest_int('min_child_weight', 1, 20), \n",
    "                          'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05), \n",
    "                          'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05),\n",
    "                          'seed': trial.suggest_int('seed', 1, 1000)\n",
    "                         }\n",
    "    \n",
    "    ## Building the model\n",
    "    xgb_md = XGBRegressor(**xgboost_param_grid, n_jobs = -1, booster = 'gbtree', tree_method = 'hist').fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test data-frame\n",
    "    xgb_md_preds = xgb_md.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    xgb_md_mse = mean_squared_error(Y_validation, xgb_md_preds, squared = False)\n",
    "    \n",
    "    return xgb_md_mse\n",
    "\n",
    "def lgbm_reg_objective(trial):\n",
    "    \n",
    "    ## Defining the LGB hyper-parameter grid\n",
    "    LGB_param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 50),\n",
    "                      'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                      'num_leaves': trial.suggest_int('num_leaves', 5, 40, step = 1),\n",
    "                      'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                      'subsample': trial.suggest_float('subsample', 0.6, 1, step = 0.05), \n",
    "                      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05),\n",
    "                      'random_state': trial.suggest_int('random_state', 1, 1000),\n",
    "                     }\n",
    "                     \n",
    "    ## Building the LightGBM model\n",
    "    model = LGBMRegressor(**LGB_param_grid, n_jobs = -1, boosting_type = 'dart', objective = 'rmse', verbosity = -1).fit(X_train, Y_train)\n",
    "        \n",
    "    ## Predicting on the test data-frame\n",
    "    lgbm_md_preds = model.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    lgbm_md_mse = mean_squared_error(Y_validation, lgbm_md_preds, squared = False)\n",
    "    \n",
    "    return lgbm_md_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877e526-3396-4a35-a96a-987692423bff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'new_time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X = train[variables]\n",
    "Y = train['fare_amount']\n",
    "\n",
    "## Splitting the data into train and validation sets\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size = 0.3)\n",
    "\n",
    "## Starting RandomForest\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_rf = optuna.create_study(direction = 'minimize')\n",
    "study_rf.optimize(rf_reg_objective, n_trials = 500)\n",
    "\n",
    "## Starting HistGradientBoosting\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_hist = optuna.create_study(direction = 'minimize')\n",
    "study_hist.optimize(hist_reg_objective, n_trials = 500)\n",
    "\n",
    "## Starting XGBoost\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_xgb = optuna.create_study(direction = 'minimize')\n",
    "study_xgb.optimize(xgb_reg_objective, n_trials = 500)\n",
    "\n",
    "## Starting LightGBM\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_lgbm = optuna.create_study(direction = 'minimize')\n",
    "study_lgbm.optimize(lgbm_reg_objective, n_trials = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70816f04-2608-4b71-ae2b-fe27c9d3170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing best hyper-parameter set\n",
    "print('Random Forest: \\n', study_rf.best_trial.params)\n",
    "print(study_rf.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('HistGB: \\n', study_hist.best_trial.params)\n",
    "print(study_hist.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nXGBoost: \\n', study_xgb.best_trial.params)\n",
    "print(study_xgb.best_trial.value)\n",
    "\n",
    "## Printing best hyper-parameter set\n",
    "print('\\nLightGBM: \\n', study_lgbm.best_trial.params)\n",
    "print(study_lgbm.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a8e68-c05c-4eb3-8729-68d11a19660f",
   "metadata": {},
   "source": [
    "#### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab9083-68b9-402f-8909-d5cff9d3d7ef",
   "metadata": {},
   "source": [
    "Round 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805b192-6eab-4c0f-b6e0-e08e0e0fef60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X = train[variables]\n",
    "Y = train['fare_amount']\n",
    "\n",
    "## Splitting the data into training and validation sets\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLM:')\n",
    "lm_train_preds = lm_md.predict(X_training); print(mean_squared_error(Y_training, lm_train_preds, squared = False))\n",
    "lm_val_preds = lm_md.predict(X_validation); print(mean_squared_error(Y_validation, lm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nRF:')\n",
    "rf_train_preds = rf_md.predict(X_training); print(mean_squared_error(Y_training, rf_train_preds, squared = False))\n",
    "rf_val_preds = rf_md.predict(X_validation); print(mean_squared_error(Y_validation, rf_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nHist:')\n",
    "hist_train_preds = hist_md.predict(X_training); print(mean_squared_error(Y_training, hist_train_preds, squared = False))\n",
    "hist_val_preds = hist_md.predict(X_validation); print(mean_squared_error(Y_validation, hist_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nXGB:')\n",
    "xgb_train_preds = XGB_md.predict(X_training); print(mean_squared_error(Y_training, xgb_train_preds, squared = False))\n",
    "xgb_val_preds = XGB_md.predict(X_validation); print(mean_squared_error(Y_validation, xgb_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLGBM:')\n",
    "lgbm_train_preds = lgb_md.predict(X_training); print(mean_squared_error(Y_training, lgbm_train_preds, squared = False))\n",
    "lgbm_val_preds = lgb_md.predict(X_validation); print(mean_squared_error(Y_validation, lgbm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Ensemble\n",
    "print('\\nEnsemble:')\n",
    "ensemble_train_preds1 = (rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 4; print(mean_squared_error(Y_training, ensemble_train_preds1, squared = False))\n",
    "ensemble_val_preds1 = (rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 4; print(mean_squared_error(Y_validation, ensemble_val_preds1, squared = False))\n",
    "\n",
    "print('\\nEnsemble 2:')\n",
    "ensemble_train_preds2 = (rf_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds2, squared = False))\n",
    "ensemble_val_preds2 = (rf_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds2, squared = False))\n",
    "\n",
    "print('\\nEnsemble 3:')\n",
    "ensemble_train_preds3 = (hist_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds3, squared = False))\n",
    "ensemble_val_preds3 = (hist_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds3, squared = False))\n",
    "\n",
    "print('\\nEnsemble 4:')\n",
    "ensemble_train_preds4 = (lm_train_preds + rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 5; print(mean_squared_error(Y_training, ensemble_train_preds4, squared = False))\n",
    "ensemble_val_preds4 = (lm_val_preds + rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 5; print(mean_squared_error(Y_validation, ensemble_val_preds4, squared = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3caebce0-4633-4f05-8c42-6710fd5b988e",
   "metadata": {},
   "source": [
    "Round 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedcd0d1-4835-4be4-9cfb-c3848b0aaca5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude_rounded', 'pickup_latitude_rounded', \n",
    "             'dropoff_longitude_rounded', 'dropoff_latitude_rounded', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X = train[variables]\n",
    "Y = train['fare_amount']\n",
    "\n",
    "## Splitting the data into training and validation sets\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLM:')\n",
    "lm_train_preds = lm_md.predict(X_training); print(mean_squared_error(Y_training, lm_train_preds, squared = False))\n",
    "lm_val_preds = lm_md.predict(X_validation); print(mean_squared_error(Y_validation, lm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nRF:')\n",
    "rf_train_preds = rf_md.predict(X_training); print(mean_squared_error(Y_training, rf_train_preds, squared = False))\n",
    "rf_val_preds = rf_md.predict(X_validation); print(mean_squared_error(Y_validation, rf_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nHist:')\n",
    "hist_train_preds = hist_md.predict(X_training); print(mean_squared_error(Y_training, hist_train_preds, squared = False))\n",
    "hist_val_preds = hist_md.predict(X_validation); print(mean_squared_error(Y_validation, hist_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nXGB:')\n",
    "xgb_train_preds = XGB_md.predict(X_training); print(mean_squared_error(Y_training, xgb_train_preds, squared = False))\n",
    "xgb_val_preds = XGB_md.predict(X_validation); print(mean_squared_error(Y_validation, xgb_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLGBM:')\n",
    "lgbm_train_preds = lgb_md.predict(X_training); print(mean_squared_error(Y_training, lgbm_train_preds, squared = False))\n",
    "lgbm_val_preds = lgb_md.predict(X_validation); print(mean_squared_error(Y_validation, lgbm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Ensemble\n",
    "print('\\nEnsemble:')\n",
    "ensemble_train_preds1 = (rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 4; print(mean_squared_error(Y_training, ensemble_train_preds1, squared = False))\n",
    "ensemble_val_preds1 = (rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 4; print(mean_squared_error(Y_validation, ensemble_val_preds1, squared = False))\n",
    "\n",
    "print('\\nEnsemble 2:')\n",
    "ensemble_train_preds2 = (rf_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds2, squared = False))\n",
    "ensemble_val_preds2 = (rf_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds2, squared = False))\n",
    "\n",
    "print('\\nEnsemble 3:')\n",
    "ensemble_train_preds3 = (hist_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds3, squared = False))\n",
    "ensemble_val_preds3 = (hist_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds3, squared = False))\n",
    "\n",
    "print('\\nEnsemble 4:')\n",
    "ensemble_train_preds4 = (lm_train_preds + rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 5; print(mean_squared_error(Y_training, ensemble_train_preds4, squared = False))\n",
    "ensemble_val_preds4 = (lm_val_preds + rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 5; print(mean_squared_error(Y_validation, ensemble_val_preds4, squared = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb74d3e-ea89-4583-bd73-63ae55111e24",
   "metadata": {},
   "source": [
    "Round 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0dde02-c21d-4419-9353-51513dc164ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "variables = ['haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', \n",
    "             'dropoff_longitude', 'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X = train[variables]\n",
    "Y = train['fare_amount']\n",
    "\n",
    "## Splitting the data into training and validation sets\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLM:')\n",
    "lm_train_preds = lm_md.predict(X_training); print(mean_squared_error(Y_training, lm_train_preds, squared = False))\n",
    "lm_val_preds = lm_md.predict(X_validation); print(mean_squared_error(Y_validation, lm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nRF:')\n",
    "rf_train_preds = rf_md.predict(X_training); print(mean_squared_error(Y_training, rf_train_preds, squared = False))\n",
    "rf_val_preds = rf_md.predict(X_validation); print(mean_squared_error(Y_validation, rf_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nHist:')\n",
    "hist_train_preds = hist_md.predict(X_training); print(mean_squared_error(Y_training, hist_train_preds, squared = False))\n",
    "hist_val_preds = hist_md.predict(X_validation); print(mean_squared_error(Y_validation, hist_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nXGB:')\n",
    "xgb_train_preds = XGB_md.predict(X_training); print(mean_squared_error(Y_training, xgb_train_preds, squared = False))\n",
    "xgb_val_preds = XGB_md.predict(X_validation); print(mean_squared_error(Y_validation, xgb_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLGBM:')\n",
    "lgbm_train_preds = lgb_md.predict(X_training); print(mean_squared_error(Y_training, lgbm_train_preds, squared = False))\n",
    "lgbm_val_preds = lgb_md.predict(X_validation); print(mean_squared_error(Y_validation, lgbm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Ensemble\n",
    "print('\\nEnsemble:')\n",
    "ensemble_train_preds1 = (rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 4; print(mean_squared_error(Y_training, ensemble_train_preds1, squared = False))\n",
    "ensemble_val_preds1 = (rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 4; print(mean_squared_error(Y_validation, ensemble_val_preds1, squared = False))\n",
    "\n",
    "print('\\nEnsemble 2:')\n",
    "ensemble_train_preds2 = (rf_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds2, squared = False))\n",
    "ensemble_val_preds2 = (rf_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds2, squared = False))\n",
    "\n",
    "print('\\nEnsemble 3:')\n",
    "ensemble_train_preds3 = (hist_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds3, squared = False))\n",
    "ensemble_val_preds3 = (hist_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds3, squared = False))\n",
    "\n",
    "print('\\nEnsemble 4:')\n",
    "ensemble_train_preds4 = (lm_train_preds + rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 5; print(mean_squared_error(Y_training, ensemble_train_preds4, squared = False))\n",
    "ensemble_val_preds4 = (lm_val_preds + rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 5; print(mean_squared_error(Y_validation, ensemble_val_preds4, squared = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e2c8a-c0a9-4c1d-b7fb-45ad68639a5e",
   "metadata": {},
   "source": [
    "Round 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f608e29a-af31-44b5-a85d-5b77ff2e91d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining the input and target variables\n",
    "variables = ['haversine', 'time_estimate', 'pickup_longitude_rounded', 'pickup_latitude_rounded', \n",
    "             'dropoff_longitude_rounded', 'dropoff_latitude_rounded', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X = train[variables]\n",
    "Y = train['fare_amount']\n",
    "\n",
    "## Splitting the data into training and validation sets\n",
    "X_training, X_validation, Y_training, Y_validation = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLM:')\n",
    "lm_train_preds = lm_md.predict(X_training); print(mean_squared_error(Y_training, lm_train_preds, squared = False))\n",
    "lm_val_preds = lm_md.predict(X_validation); print(mean_squared_error(Y_validation, lm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nRF:')\n",
    "rf_train_preds = rf_md.predict(X_training); print(mean_squared_error(Y_training, rf_train_preds, squared = False))\n",
    "rf_val_preds = rf_md.predict(X_validation); print(mean_squared_error(Y_validation, rf_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nHist:')\n",
    "hist_train_preds = hist_md.predict(X_training); print(mean_squared_error(Y_training, hist_train_preds, squared = False))\n",
    "hist_val_preds = hist_md.predict(X_validation); print(mean_squared_error(Y_validation, hist_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nXGB:')\n",
    "xgb_train_preds = XGB_md.predict(X_training); print(mean_squared_error(Y_training, xgb_train_preds, squared = False))\n",
    "xgb_val_preds = XGB_md.predict(X_validation); print(mean_squared_error(Y_validation, xgb_val_preds, squared = False))\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_training, Y_training)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "print('\\nLGBM:')\n",
    "lgbm_train_preds = lgb_md.predict(X_training); print(mean_squared_error(Y_training, lgbm_train_preds, squared = False))\n",
    "lgbm_val_preds = lgb_md.predict(X_validation); print(mean_squared_error(Y_validation, lgbm_val_preds, squared = False))\n",
    "\n",
    "\n",
    "\n",
    "## Ensemble\n",
    "print('\\nEnsemble:')\n",
    "ensemble_train_preds1 = (rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 4; print(mean_squared_error(Y_training, ensemble_train_preds1, squared = False))\n",
    "ensemble_val_preds1 = (rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 4; print(mean_squared_error(Y_validation, ensemble_val_preds1, squared = False))\n",
    "\n",
    "print('\\nEnsemble 2:')\n",
    "ensemble_train_preds2 = (rf_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds2, squared = False))\n",
    "ensemble_val_preds2 = (rf_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds2, squared = False))\n",
    "\n",
    "print('\\nEnsemble 3:')\n",
    "ensemble_train_preds3 = (hist_train_preds + xgb_train_preds + lgbm_train_preds) / 3; print(mean_squared_error(Y_training, ensemble_train_preds3, squared = False))\n",
    "ensemble_val_preds3 = (hist_val_preds + xgb_val_preds + lgbm_val_preds) / 3; print(mean_squared_error(Y_validation, ensemble_val_preds3, squared = False))\n",
    "\n",
    "print('\\nEnsemble 4:')\n",
    "ensemble_train_preds4 = (lm_train_preds + rf_train_preds + hist_train_preds + xgb_train_preds + lgbm_train_preds) / 5; print(mean_squared_error(Y_training, ensemble_train_preds4, squared = False))\n",
    "ensemble_val_preds4 = (lm_val_preds + rf_val_preds + hist_val_preds + xgb_val_preds + lgbm_val_preds) / 5; print(mean_squared_error(Y_validation, ensemble_val_preds4, squared = False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bc1416-7aa2-4f31-85b6-ea9edc0803e0",
   "metadata": {},
   "source": [
    "#### Final Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefc845-2a13-443c-bdef-41a7a560b37c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Linear Model\n",
    "\n",
    "## Subsetting the data\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "lm_train_preds = lm_md.predict(X_train)\n",
    "lm_test_preds = lm_md.predict(X_test)\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = lm_test_preds\n",
    "sub.to_csv('Submissions/linear_model_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30001f9-6738-4aba-bec4-a68c8f812b6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "\n",
    "## Subsetting the data\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "rf_train_preds = rf_md.predict(X_train)\n",
    "rf_test_preds = rf_md.predict(X_test)\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = rf_test_preds\n",
    "sub.to_csv('Submissions/random_forest_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a0a74-ba15-430e-9ef6-57dfdb390248",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## HistGradientBoosting\n",
    "\n",
    "## Subsetting the data\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "hist_train_preds = hist_md.predict(X_train)\n",
    "hist_test_preds = hist_md.predict(X_test)\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = hist_test_preds\n",
    "sub.to_csv('Submissions/hist_gb_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5012b6e-7814-4caf-aa32-0700c802acf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## XGBoost\n",
    "\n",
    "## Subsetting the data\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "xgb_train_preds = XGB_md.predict(X_train)\n",
    "xgb_test_preds = XGB_md.predict(X_test)\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = xgb_test_preds\n",
    "sub.to_csv('Submissions/xgb_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e5cdd8-a819-4939-897a-30264aa3a1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## LightGBM\n",
    "\n",
    "## Subsetting the data\n",
    "variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "             'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "lgbm_train_preds = lgb_md.predict(X_train)\n",
    "lgbm_test_preds = lgb_md.predict(X_test)\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = lgbm_test_preds\n",
    "sub.to_csv('Submissions/lgbm_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992021af-f79f-4709-9bc6-c68c73e1df63",
   "metadata": {},
   "source": [
    "#### Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f0e366-28f2-4e08-a243-8edf8995fa30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Ensemble from round 1:\n",
    "\n",
    "## Defining the input and target variables\n",
    "# variables = ['distance', 'duration', 'haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', \n",
    "#              'dropoff_latitude', 'dropoff_other', 'change_borough', 'LGA']\n",
    "variables = ['haversine', 'time_estimate', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "\n",
    "\n",
    "X_train = train[variables]\n",
    "Y_train = train['fare_amount']\n",
    "\n",
    "X_test = test[variables]\n",
    "\n",
    "\n",
    "## Linear Model\n",
    "\n",
    "## Building the model\n",
    "lm_md = LinearRegression(n_jobs = -1).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "lm_train_preds = lm_md.predict(X_train)\n",
    "lm_test_preds = lm_md.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "## Random Forest\n",
    "\n",
    "## Building the model\n",
    "rf_md = RandomForestRegressor(max_depth = 11, n_estimators = 850, min_samples_split = 12, min_samples_leaf = 2, \n",
    "                             max_features = 'sqrt', random_state = 372, n_jobs = -1, criterion = 'squared_error').fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "rf_train_preds = rf_md.predict(X_train)\n",
    "rf_test_preds = rf_md.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "## Building the model\n",
    "hist_md = HistGradientBoostingRegressor(learning_rate = 0.16, max_iter = 300, max_depth = 3, loss = 'squared_error', l2_regularization = 0.078, \n",
    "                                        early_stopping = True, random_state = 115).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "hist_train_preds = hist_md.predict(X_train)\n",
    "hist_test_preds = hist_md.predict(X_test)\n",
    "\n",
    "\n",
    "## XGBoost\n",
    "\n",
    "## Building the model\n",
    "XGB_md = XGBRegressor(tree_method = 'hist',  booster = 'gbtree', colsample_bytree = 1, gamma = 0.2, learning_rate = 0.04, max_depth = 4, \n",
    "                      min_child_weight = 5, n_estimators = 250, subsample = 0.6, n_jobs = -1, seed = 712).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "xgb_train_preds = XGB_md.predict(X_train)\n",
    "xgb_test_preds = XGB_md.predict(X_test)\n",
    "\n",
    "\n",
    "## LightGBM\n",
    "\n",
    "## Building the model\n",
    "lgb_md = LGBMRegressor(n_estimators = 650, max_depth = 3, learning_rate = 0.24, num_leaves = 8, subsample = 1, \n",
    "                       colsample_bytree = 1, random_state = 949).fit(X_train, Y_train)\n",
    "\n",
    "## Predicting on the training and testing sets\n",
    "lgbm_train_preds = lgb_md.predict(X_train)\n",
    "lgbm_test_preds = lgb_md.predict(X_test)\n",
    "\n",
    "\n",
    "\n",
    "## Building the ensemble models\n",
    "ensemble_test_preds1 = (rf_test_preds + hist_test_preds + xgb_test_preds + lgbm_test_preds) / 4\n",
    "ensemble_test_preds2 = (rf_test_preds + xgb_test_preds + lgbm_test_preds) / 3\n",
    "ensemble_test_preds3 = (hist_test_preds + xgb_test_preds + lgbm_test_preds) / 3\n",
    "ensemble_test_preds4 = (lm_test_preds + rf_test_preds + hist_test_preds + xgb_test_preds + lgbm_test_preds) / 5\n",
    "\n",
    "## Finalizing competition submission\n",
    "sub['fare_amount'] = ensemble_test_preds1\n",
    "sub['fare_amount'] = np.where(sub['fare_amount'] < 2.5, 2.5, sub['fare_amount'])\n",
    "sub.to_csv('Submissions/ensemble_submission1.csv', index = False)\n",
    "\n",
    "sub['fare_amount'] = ensemble_test_preds2\n",
    "sub['fare_amount'] = np.where(sub['fare_amount'] < 2.5, 2.5, sub['fare_amount'])\n",
    "sub.to_csv('Submissions/ensemble_submission2.csv', index = False)\n",
    "\n",
    "sub['fare_amount'] = ensemble_test_preds3\n",
    "sub['fare_amount'] = np.where(sub['fare_amount'] < 2.5, 2.5, sub['fare_amount'])\n",
    "sub.to_csv('Submissions/ensemble_submission3.csv', index = False)\n",
    "\n",
    "sub['fare_amount'] = ensemble_test_preds4\n",
    "sub['fare_amount'] = np.where(sub['fare_amount'] < 2.5, 2.5, sub['fare_amount'])\n",
    "sub.to_csv('Submissions/ensemble_submission4.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
