{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5df555-0ae9-4572-8f89-92454d6eb20b",
   "metadata": {},
   "source": [
    "### W23P1 STAT 857 - Hyper-Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a250a9f1-6217-456b-a723-66bd337e2bf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install optuna lightgbm xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d447879f-1a98-41c5-87ea-0e3b5a696129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Importing libraries\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "np.seterr(divide='ignore', invalid='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e533aa7-6642-4959-94e1-532e77f5297a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "data = pd.read_csv('Data/W23P1_train_final.csv')\n",
    "\n",
    "## Defining the input and target variables\n",
    "\n",
    "variables = ['passenger_count', 'distance', 'duration', 'pickup_day', 'pickup_hour', 'Friday', 'Monday', 'Saturday', \n",
    "             'Sunday', 'Thursday', 'Tuesday', 'Wednesday', 'weekend', 'rush_hour', 'overnight', 'pickup_LGA', 'dropoff_LGA', \n",
    "             'pickup_JFK', 'dropoff_JFK', 'pickup_EWR', 'dropoff_EWR', 'airport', 'change_borough', 'haversine']\n",
    "\n",
    "X = data[variables]\n",
    "Y = data['fare_amount']\n",
    "\n",
    "## Splitting the data into train and validation sets\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70888978-7672-4e94-a323-d3d647af104a",
   "metadata": {},
   "source": [
    "#### Defining optuna objective functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8965c53-479b-45fc-83f9-0d3d50fb0ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rf_reg_objective(trial):\n",
    "\n",
    "    ## Defining the XGBoost hyper-parameter grid\n",
    "    rf_param_grid = {'n_estimators': trial.suggest_int('n_estimators', 100, 1000, 100),\n",
    "                     'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                     'min_samples_split': trial.suggest_int('min_samples_split', 2, 10), \n",
    "                     'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10), \n",
    "                    }\n",
    "    \n",
    "    ## Building the model\n",
    "    rf_md = RandomForestRegressor(**rf_param_grid, n_jobs = -1).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test data-frame\n",
    "    rf_md_preds = rf_md.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    rf_md_mse = mean_squared_error(Y_validation, rf_md_preds, squared = False)\n",
    "    \n",
    "    return rf_md_mse\n",
    "\n",
    "def xgb_reg_objective(trial):\n",
    "\n",
    "    ## Defining the XGBoost hyper-parameter grid\n",
    "    xgboost_param_grid = {'tree_method':'hist', \n",
    "                          'n_estimators': trial.suggest_int('n_estimators', 100, 500, 100), \n",
    "                          'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01), \n",
    "                          'max_depth': trial.suggest_int('max_depth', 3, 12), \n",
    "                          'gamma': trial.suggest_float('gamma', 0.01, 0.3, step = 0.01), \n",
    "                          'min_child_weight': trial.suggest_int('min_child_weight', 5, 15), \n",
    "                          'subsample': trial.suggest_float('subsample', 0.7, 1, step = 0.01), \n",
    "                          'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1, step = 0.01)\n",
    "                         }\n",
    "    \n",
    "    ## Building the model\n",
    "    xgb_md = XGBRegressor(**xgboost_param_grid, n_jobs = -1).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on the test data-frame\n",
    "    xgb_md_preds = xgb_md.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    xgb_md_mse = mean_squared_error(Y_validation, xgb_md_preds, squared = False)\n",
    "    \n",
    "    return xgb_md_mse\n",
    "\n",
    "def lgbm_reg_objective(trial):\n",
    "    \n",
    "    ## Defining the LGB hyper-parameter grid\n",
    "    LGB_param_grid = {'boosting_type': 'dart',\n",
    "                      'n_estimators': trial.suggest_int('n_estimators', 100, 1500, 100),\n",
    "                      'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                      'num_leaves': trial.suggest_int('num_leaves', 5, 40, step = 1),\n",
    "                      'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                      'subsample': trial.suggest_float('subsample', 0.7, 1, step = 0.01), \n",
    "                      'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1, step = 0.01),\n",
    "                      'random_state': trial.suggest_int('random_state', 1, 1000),\n",
    "                      'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 0.1, step = 0.001),\n",
    "                      'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 0.1, step = 0.001), \n",
    "                      'objective': 'rmse', \n",
    "                      'verbosity': -1\n",
    "                     }\n",
    "                     \n",
    "    ## Building the LightGBM model\n",
    "    model = LGBMRegressor(**LGB_param_grid, n_jobs = -1).fit(X_train, Y_train)\n",
    "        \n",
    "    ## Predicting on the test data-frame\n",
    "    lgbm_md_preds = model.predict(X_validation)\n",
    "    \n",
    "    ## Evaluating model performance on the test set\n",
    "    lgbm_md_mse = mean_squared_error(Y_validation, lgbm_md_preds, squared = False)\n",
    "    \n",
    "    return lgbm_md_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d90b14-d236-42f9-96dd-8f9682279a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Starting RandomForest\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_rf = optuna.create_study(direction = 'minimize')\n",
    "study_rf.optimize(rf_reg_objective, n_trials = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29978412-bb5b-4dde-8d01-bc54b0626b19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 300, 'max_depth': 12, 'min_samples_split': 5, 'min_samples_leaf': 6}\n",
      "3.36698326576257\n"
     ]
    }
   ],
   "source": [
    "## Printing best hyper-parameter set\n",
    "print(study_rf.best_trial.params)\n",
    "\n",
    "## Printing model performance\n",
    "print(study_rf.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a0446-390a-449c-ade0-f4e316b2f2a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Starting XGBoost\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_xgb = optuna.create_study(direction = 'minimize')\n",
    "study_xgb.optimize(xgb_reg_objective, n_trials = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23b4cf81-6c6e-4b5c-8102-308e0cbdebf2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 500, 'learning_rate': 0.02, 'max_depth': 5, 'gamma': 0.2, 'min_child_weight': 10, 'subsample': 0.94, 'colsample_bytree': 0.9199999999999999}\n",
      "3.2904768024142927\n"
     ]
    }
   ],
   "source": [
    "## Printing best hyper-parameter set\n",
    "print(study_xgb.best_trial.params)\n",
    "\n",
    "## Printing model performance\n",
    "print(study_xgb.best_trial.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78abec8c-983b-455c-b282-2d8f06d5d6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Starting LightGBM\n",
    "## ----\n",
    "## Creating a study object and to optimize the home objective function\n",
    "study_lgbm = optuna.create_study(direction = 'minimize')\n",
    "study_lgbm.optimize(lgbm_reg_objective, n_trials = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65376f56-4ea6-4753-9980-fdbafebc9cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 600, 'learning_rate': 0.18000000000000002, 'num_leaves': 8, 'max_depth': 8, 'subsample': 0.73, 'colsample_bytree': 0.86, 'random_state': 543, 'reg_alpha': 0.021, 'reg_lambda': 0.027000000000000003}\n",
      "3.3219108358489633\n"
     ]
    }
   ],
   "source": [
    "## Printing best hyper-parameter set\n",
    "print(study_lgbm.best_trial.params)\n",
    "\n",
    "## Printing model performance\n",
    "print(study_lgbm.best_trial.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
