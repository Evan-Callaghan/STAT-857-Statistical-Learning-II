{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e39d24-3509-4473-acc2-ca84eabde279",
   "metadata": {},
   "source": [
    "# STAT 857 - W23 Project 2\n",
    "## Evan Callaghan | April 17, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0845314e-790d-4c66-8eff-604e4f8f3fba",
   "metadata": {},
   "source": [
    "### 1. Configuring setup\n",
    "Installing packages and loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40914fa-da12-475b-b755-33ca677ef231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lightgbm xgboost optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640dde3d-1769-4f98-9c86-69b460a9d1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import math\n",
    "import optuna\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_selection import RFECV, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec184436-47c9-4336-a042-c170121b9d33",
   "metadata": {},
   "source": [
    "### 2. Data Exploration Section\n",
    "\n",
    "Reading the competition data files, exploring the training data \\\n",
    "Note: all data visualizations created for the final report were done in Tableau using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5b6df-4fbc-41d5-8052-cdadfc822ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "train = pd.read_csv('Data/W23P2_train.csv')\n",
    "test = pd.read_csv('Data/W23P2_test.csv')\n",
    "sub = pd.read_csv('Data/Sample_Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57a7466-9803-4bd8-8c9c-2b1fcb269491",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96b25ce-90cc-4cdf-a309-a5f23a96b686",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0bc3b-401d-4692-8741-0bd49c684748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f0699c-a325-466d-9edd-76560dc2b291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train['interest_level'].value_counts() / train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100d100a-1360-4b6d-b0f2-8420a3b98c8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8956a9-47d8-49fc-9510-2f1e31822782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41324712-e948-4e79-b21c-5b26498247a5",
   "metadata": {},
   "source": [
    "### 3. Data Cleaning\n",
    "Removing some outlier observations and cleaning street_address variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5aee24-af25-4396-87cb-53484ef8429c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Transforming interest_level label to numeric\n",
    "train['interest_level'] = np.where(train['interest_level'] == 'low', 0, \n",
    "                                   np.where(train['interest_level'] == 'medium', 1, 2))\n",
    "\n",
    "## Removing training observations with prices higher than $40,000\n",
    "train = train[train['price'] < 40000].reset_index(drop = True)\n",
    "\n",
    "## Removing outlier locations\n",
    "train = train[(train['latitude'] < 43) & (train['latitude'] > 1)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa3cd5a-b1e6-4c31-9924-f31a4f116d23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Combining data frames for cleaning and feature engineering purposes\n",
    "train.insert(0, 'train', 1)\n",
    "test.insert(0, 'train', 0)\n",
    "full_data = pd.concat([train.drop(columns = ['interest_level']), test.drop(columns = ['ID'])])\n",
    "\n",
    "## Making sure address variable is a string\n",
    "full_data['street_address'] = full_data['street_address'].astype(str)\n",
    "\n",
    "## Altering the missing values\n",
    "full_data['street_address'] = np.where(full_data['street_address'] == 'nan', '123 nan', full_data['street_address'])\n",
    "\n",
    "## Changing street address variable to all lowercase\n",
    "full_data['street_address'] = full_data['street_address'].str.lower() \n",
    "\n",
    "## Splitting street_address into number and street\n",
    "addies = pd.DataFrame(list(full_data['street_address'].str.split(' ', 1)), columns = ['number', 'street'])\n",
    "addies['street'] = ' ' + addies['street'] + ' '\n",
    "\n",
    "## Cleaning street variable\n",
    "addies['street'] = addies['street'].str.replace(' st ', ' street', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' st. ', ' street', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' blvd ', ' boulevard ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ave ', ' avenue ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ave. ', ' avenue ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' dr ', ' drive ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' pl ', ' place ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' e ', ' east ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' w ', ' west ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' first ', ' 1st ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' second ', ' 2nd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' third ', ' 3rd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' fourth ', ' 4th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' fifth ', ' 5th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' sixth ', ' 6th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' seventh ', ' 7th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' eighth ', ' 8th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ninth ', ' 9th ', regex = True)\n",
    "\n",
    "addies['street'] = addies['street'].str.replace('1 ', '1st ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('2 ', '2nd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('3 ', '3rd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('4 ', '4th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('5 ', '5th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('6 ', '6th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('7 ', '7th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('8 ', '8th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('9 ', '9th ', regex = True)\n",
    "\n",
    "\n",
    "## Adding cleaned street_address variable back into the full_data data-frame\n",
    "full_data['street_address'] = addies['number'] + ' ' + addies['street']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709c386-30e7-4bfc-8ac0-0d520d7f3991",
   "metadata": {},
   "source": [
    "### 4. Variable Engineering\n",
    "Creating new variables based on provided amenities and apartment pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e8682b-c85e-4da1-95bb-fb7964bd4ccd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding geolocation features\n",
    "full_data['geo_area_50'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 50) % 50) * 50 + (int(-x[1] * 50) % 50), axis = 1)                                         \n",
    "                         \n",
    "full_data['geo_area_100'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 100) % 100) * 100 + (int(-x[1] * 100) % 100), axis = 1)                                         \n",
    "  \n",
    "full_data['geo_area_200'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 200) % 200) * 200 + (int(-x[1] * 200) % 200), axis = 1)                                         \n",
    "\n",
    "## Financial district\n",
    "lat = 40.705628\n",
    "lon = -74.010278\n",
    "full_data['distance_to_fin'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:math.sqrt((x[0] - lat)**2 + (x[1] - lon)**2), axis = 1)\n",
    "\n",
    "## Central park\n",
    "lat = 40.785091\n",
    "lon = -73.968285\n",
    "full_data['distance_to_cp'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:math.sqrt((x[0] - lat)**2 + (x[1] - lon)**2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b75504-fdb3-4e6c-a41a-d4c64c28a8a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Log-transformation of the price variable\n",
    "full_data['log_price'] = np.log(full_data['price'])\n",
    "\n",
    "## Number of total rooms\n",
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "\n",
    "## Number of categorical features listed \n",
    "full_data['num_of_features'] = full_data.iloc[:, 7:67].sum(axis = 1)\n",
    "\n",
    "## Number of half-baths\n",
    "full_data['half_bathrooms'] = full_data['bathrooms'] - full_data['bathrooms'].apply(int)\n",
    "\n",
    "## Price per room\n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per bedroom\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per bathroom\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per feature listed\n",
    "full_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per room\n",
    "full_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per bedroom\n",
    "full_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per bathroom\n",
    "full_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d75c7d-4e2a-444e-930e-bb0c86ef71c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Making sure address variable is a string\n",
    "full_data['street_address'] = full_data['street_address'].astype(str)\n",
    "\n",
    "## Getting a count of observations with same address\n",
    "street = full_data['street_address'].value_counts()\n",
    "\n",
    "## Getting a count of observations with same number of bedrooms\n",
    "bedrooms = full_data['bedrooms'].value_counts()\n",
    "\n",
    "## Getting a count of observations with same number of bathrooms\n",
    "bathrooms = full_data['bathrooms'].value_counts()\n",
    "\n",
    "## Adding count information\n",
    "full_data['street_count'] = full_data['street_address'].apply(lambda x:street[x] if x == 'nan' else street[x])\n",
    "full_data['bedrooms_count'] = full_data['bedrooms'].apply(lambda x:bedrooms[x])\n",
    "full_data['bathrooms_count'] = full_data['bathrooms'].apply(lambda x:bathrooms[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cabf02c-9cd7-4c25-96e9-f70fd9251414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Computing density - number of listings in given area\n",
    "full_data['pos'] = full_data['longitude'].round(3).astype(str) + '_' + full_data['latitude'].round(3).astype(str)\n",
    "vals = full_data['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "full_data['density'] = full_data['pos'].apply(lambda x: dvals.get(x, vals.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c740134-01fa-46c9-a96d-3b76c41c4e29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Aggregating price variables by street address and computing summary statistics\n",
    "price_by_address = full_data.groupby('street_address')['price'].agg([np.min, np.max, np.median, np.mean]).reset_index()\n",
    "price_by_address.columns = ['street_address','min_price_by_address',\n",
    "                            'max_price_by_address','median_price_by_address','mean_price_by_address']\n",
    "\n",
    "## Adding aggregated price info\n",
    "full_data = pd.merge(full_data, price_by_address, how = 'left', on = 'street_address')\n",
    "\n",
    "## Computing percentile from aggregated price info\n",
    "full_data['price_percentile_by_address'] = full_data[['price','min_price_by_address','max_price_by_address']]\\\n",
    ".apply(lambda x:(x[0] - x[1]) / (x[2] - x[1]) if (x[2] - x[1]) != 0 else 0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a348ba3-c014-48f6-aaa4-29fab302fedf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Aggregating price variables by position and computing summary statistics\n",
    "price_by_pos = full_data.groupby('pos')['price'].agg([np.min, np.max, np.median, np.mean]).reset_index()\n",
    "price_by_pos.columns = ['pos','min_price_by_pos',\n",
    "                            'max_price_by_pos','median_price_by_pos','mean_price_by_pos']\n",
    "\n",
    "## Adding aggregated price info\n",
    "full_data = pd.merge(full_data, price_by_pos, how = 'left', on = 'pos')\n",
    "\n",
    "## Computing percentile from aggregated price info\n",
    "full_data['price_percentile_by_pos'] = full_data[['price','min_price_by_pos','max_price_by_pos']]\\\n",
    ".apply(lambda x:(x[0] - x[1]) / (x[2] - x[1]) if (x[2] - x[1]) != 0 else 0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f580b0-b092-4582-9d05-06981fc2dbec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Computing market price for combination of address, bedrooms, and bathrooms\n",
    "mkt_price = full_data.groupby(['street_address', 'bedrooms', 'bathrooms']).price.mean().reset_index()\n",
    "\n",
    "## Adding market price information\n",
    "mkt_price = pd.merge(full_data[['street_address', 'bedrooms', 'bathrooms']], mkt_price, how = 'left', \n",
    "                     on = ['street_address', 'bedrooms', 'bathrooms']).price\n",
    "full_data['mkt_price'] = mkt_price.values\n",
    "\n",
    "## Computing the difference to market price\n",
    "full_data['diff_to_mkt_price'] = full_data['price'] - full_data['mkt_price']\n",
    "\n",
    "## Computing the ratio to market price\n",
    "full_data['ratio_to_mkt_price'] = full_data['price'] / full_data['mkt_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379137b1-0833-4086-ace2-741a02c08710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Condensing redundant variables after feature engineering\n",
    "\n",
    "laundry_vars = ['Laundry.in.Building', 'Laundry.in.Unit', 'Laundry.In.Building', 'Laundry.In.Unit', 'LAUNDRY', 'Washer.in.Unit', \n",
    "                'Dryer.in.Unit', 'Laundry.Room', 'Laundry', 'On.site.laundry', 'On.site.Laundry', 'Washer.Dryer', 'Washer.Dryer.in.building', \n",
    "                'In.Unit.Washer.Dryer', 'Washer...Dryer', 'Washer.Dryer.in.Unit']\n",
    "parking_vars = ['Parking.Space', 'Garage', 'Parking', 'On.site.Garage', 'assigned.parking.space', 'Common.parking.Garage', 'Full.Service.Garage', \n",
    "               'On.site.Parking.Lot', 'Private.parking']\n",
    "valet_vars = ['Valet.Parking', 'Valet']\n",
    "deck_vars = ['Roof.Deck', 'Balcony', 'Terrace', 'Patio', 'Roof.deck', 'balcony', 'terrace', 'patio', 'private.balcony', 'Private.balcony', \n",
    "             'Private.Deck', 'Common.roof.deck', 'ROOFDECK']\n",
    "outdoor_vars = ['Courtyard', 'Outdoor.Entertainment.Space', 'Private.Outdoor.Space', 'private.outdoor.space', 'Private.outdoor.space', \n",
    "                'Common.Outdoor.Space', 'PublicOutdoor', 'Outdoor.Space', 'Outdoor.Areas', 'Common.backyard', 'building.common.outdoor.space']\n",
    "garden_vars = ['Common.garden', 'garden', 'Garden.Patio', 'Garden', 'Residents.Garden']\n",
    "dishwasher_vars = ['Dishwasher', 'dishwasher']\n",
    "gym_vars = ['Fitness.Center', 'Gym.Fitness', 'Health.Club', 'Gym', 'gym', 'Gym.In.Building']\n",
    "pool_vars = ['Swimming.Pool', 'Pool', 'pool', 'Indoor.Pool']\n",
    "elevator_vars = ['Elevator', 'elevator']\n",
    "storage_vars = ['Storage', 'storage', 'Basement.Storage']\n",
    "internet_vars = ['High.Speed.Internet', 'WiFi', 'WiFi.Access']\n",
    "bike_vars = ['Bike.room', 'Bike.Room']\n",
    "pet_friendly_vars = ['Dogs.Allowed', 'Cats.Allowed', 'Pet.Friendly', 'Pets.on.approval']\n",
    "concierge_vars = ['Concierge', 'Concierge.Service', 'X24.7.Concierge']\n",
    "doorman_vars = ['Doorman', 'Full.time.doorman', 'Virtual.Doorman', 'FT.Doorman', 'doorman']\n",
    "super_vars = ['LIVE.IN.SUPER', 'Live.in.superintendent', 'Live.In.Superintendent','Live.in.Super', 'Live.In.Super']\n",
    "hardwood_vars = ['Hardwood.Floors', 'HARDWOOD', 'Hardwood.floors', 'Hardwood']\n",
    "ceiling_vars = ['High.ceilings', 'High.Ceilings', 'HIGH.CEILINGS', 'High.Ceiling']\n",
    "brick_vars = ['EXPOSED.BRICK', 'Exposed.Brick']\n",
    "construction_vars = ['New.Construction', 'Newly.renovated','Renovated', 'renovated', 'New.construction']\n",
    "photo_vars = ['Actual.Apt..Photos', 'ACTUAL.APT..PHOTOS']\n",
    "lounge_vars = ['Residents.Lounge', 'Lounge.room', 'Lounge']\n",
    "playroom_vars = ['Childrens.Playroom', 'Children.s.Playroom']\n",
    "ac_vars = ['Central.A.C', 'Air.conditioning']\n",
    "kitchen_vars = ['EAT.IN.KITCHEN','Eat.In.Kitchen']\n",
    "no_fee_vars = ['No.Fee', 'NO.FEE']\n",
    "accessibity_vars = ['Wheelchair.Ramp', 'Wheelchair.Access']\n",
    "multi_level_vars = ['Multi.Level', 'Multi.level']\n",
    "fireplace_vars = ['Fireplace', 'Decorative.Fireplace']\n",
    "highrise_vars = ['Hi.Rise', 'HIGHRISE']\n",
    "marble_bath_vars = ['Marble.Bath', 'Marble.Bathroom']\n",
    "prewar_vars = ['Pre.War', 'prewar', 'Prewar']\n",
    "\n",
    "def condense(data):\n",
    "    \n",
    "    ## Condensing data\n",
    "    data['Has_Laundry'] = np.where(np.sum(data[laundry_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Parking'] = np.where(np.sum(data[parking_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Valet'] = np.where(np.sum(data[valet_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Deck'] = np.where(np.sum(data[deck_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Outdoor_Area'] = np.where(np.sum(data[outdoor_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Garden'] = np.where(np.sum(data[garden_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Dishwasher'] = np.where(np.sum(data[dishwasher_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Gym'] = np.where(np.sum(data[gym_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Pool'] = np.where(np.sum(data[pool_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Elevator'] = np.where(np.sum(data[elevator_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Storage'] = np.where(np.sum(data[storage_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Wifi'] = np.where(np.sum(data[internet_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Bike'] = np.where(np.sum(data[bike_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Pet_Friendly'] = np.where(np.sum(data[pet_friendly_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Concierge'] = np.where(np.sum(data[concierge_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Doorman'] = np.where(np.sum(data[doorman_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Super'] = np.where(np.sum(data[super_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Hardwood_Floor'] = np.where(np.sum(data[hardwood_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_High_Ceilings'] = np.where(np.sum(data[ceiling_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Brick'] = np.where(np.sum(data[brick_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Renovated'] = np.where(np.sum(data[construction_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Photos'] = np.where(np.sum(data[photo_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Lounge'] = np.where(np.sum(data[lounge_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Playroom'] = np.where(np.sum(data[playroom_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_AC'] = np.where(np.sum(data[ac_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Kitchen'] = np.where(np.sum(data[kitchen_vars], axis = 1) > 0, 1, 0)\n",
    "    data['No_Fee'] = np.where(np.sum(data[no_fee_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Accessible'] = np.where(np.sum(data[accessibity_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Multi_Level'] = np.where(np.sum(data[multi_level_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Fire'] = np.where(np.sum(data[fireplace_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Highrise'] = np.where(np.sum(data[highrise_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Marble_Bath'] = np.where(np.sum(data[marble_bath_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Pre_War'] = np.where(np.sum(data[prewar_vars], axis = 1) > 0, 1, 0)\n",
    "    return data\n",
    "    \n",
    "## Applying function to full_data\n",
    "full_data = condense(full_data)\n",
    "\n",
    "## Dropping unnecessary columns\n",
    "to_drop = [laundry_vars, parking_vars, valet_vars, deck_vars, outdoor_vars,garden_vars, dishwasher_vars, gym_vars, pool_vars, elevator_vars, \n",
    "           storage_vars, internet_vars, bike_vars, pet_friendly_vars, concierge_vars, doorman_vars, super_vars, hardwood_vars, ceiling_vars, \n",
    "           brick_vars, construction_vars, photo_vars, lounge_vars, playroom_vars, ac_vars, kitchen_vars, no_fee_vars, accessibity_vars, \n",
    "           multi_level_vars, fireplace_vars, highrise_vars, marble_bath_vars, prewar_vars] \n",
    "\n",
    "for cols in to_drop:\n",
    "    full_data = full_data.drop(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640c99c4-1e59-404a-9e8d-b22306ee9a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Label encoding categorical variables\n",
    "cat_vars = ['Dining.Room', 'Reduced.Fee', 'Exclusive', 'No.pets', 'LOWRISE', 'SIMPLEX', 'Furnished', 'Loft', 'Stainless.Steel.Appliances', 'View', \n",
    "            'Green.Building', 'Short.Term.Allowed', 'Subway', 'Granite.Kitchen', 'Light', 'Guarantors.Accepted', 'Skylight', 'Sauna', 'Live.Work', \n",
    "            'Duplex', 'Walk.in.Closet.s.', 'Luxury.building', 'Post.War', 'Cable.Satellite.TV', 'Microwave', 'Sublet', 'Shares.OK', 'Has_Laundry', \n",
    "            'Has_Parking', 'Has_Valet', 'Has_Deck', 'Has_Outdoor_Area', 'Has_Garden', 'Has_Dishwasher', 'Has_Gym', 'Has_Pool', 'Has_Elevator', \n",
    "            'Has_Storage', 'Has_Wifi', 'Has_Bike', 'Pet_Friendly', 'Has_Concierge', 'Has_Doorman', 'Has_Super', 'Has_Hardwood_Floor', \n",
    "            'Has_High_Ceilings', 'Has_Brick', 'Has_Renovated', 'Has_Photos', 'Has_Lounge', 'Has_Playroom', 'Has_AC', 'Has_Kitchen', 'No_Fee', \n",
    "            'Accessible', 'Multi_Level', 'Fire', 'Highrise', 'Marble_Bath', 'Pre_War']\n",
    "LE_vars = []\n",
    "for cat_var in cat_vars:\n",
    "    LE_var = cat_var\n",
    "    full_data[LE_var] = LabelEncoder().fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    \n",
    "## OneHot Encoding all catategorical variables\n",
    "oh_encoder = OneHotEncoder(sparse_output = True).fit(full_data[LE_vars])\n",
    "oh_sparse = oh_encoder.transform(full_data[LE_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d009b-f67d-4573-89e2-60e943e1e6ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Splitting full_data back into training and testing sets\n",
    "training = full_data[full_data['train'] == 1].drop(columns = ['train', 'street_address', 'pos']).reset_index(drop = True)\n",
    "training['interest_level'] = train['interest_level']\n",
    "\n",
    "testing = full_data[full_data['train'] == 0].drop(columns = ['train', 'street_address', 'pos']).reset_index(drop = True)\n",
    "testing['ID'] = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaaef02-bd16-47fd-b1c2-a7f6538a55d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe12e3-6bef-4ea1-8beb-1c596b55b3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9070ea-e66a-42e8-b13c-e73c32be3dba",
   "metadata": {},
   "source": [
    "### 4. Feature Selection\n",
    "Performing recursive feature elimination to determine which variables are the most influential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53077571-bb1b-4519-b949-6a933c60018b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Using RFE with RandomForestClassifer to identify most important features\n",
    "def flat_list(my_list):\n",
    "    \n",
    "    ## Defining list to store results\n",
    "    out_list = list()\n",
    "    for i in my_list:\n",
    "        out_list += i\n",
    "    return out_list\n",
    "\n",
    "def RF_RFE_rep_cross_val(X, Y, numb_folds, max_features, numb_reps):\n",
    "    \n",
    "    ## Defining list to store results\n",
    "    RFE_rep_results = list()\n",
    "    for i in range(2, max_features):\n",
    "        RFE_rep_results.append(RF_rep_cross_val(X, Y, numb_folds, i, numb_reps))\n",
    "        print('Features -->', i) ## Sanity check\n",
    "    return RFE_rep_results\n",
    "\n",
    "def RF_rep_cross_val(X, Y, numb_folds, numb_features, numb_reps):\n",
    "    \n",
    "    ## Defining the list to store results\n",
    "    rep_results = list()\n",
    "    for i in range(0, numb_reps):\n",
    "        rep_results.append(RF_cross_val(X, Y, numb_folds, numb_features))\n",
    "    return flat_list(rep_results)\n",
    "\n",
    "def RF_cross_val(X, Y, numb_folds, numb_features):\n",
    "    \n",
    "    ## Defining list to store results\n",
    "    results = list()\n",
    "    \n",
    "    ## Defining the number of folds\n",
    "    kf = KFold(n_splits = numb_folds, shuffle = True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        \n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        Y_train, Y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "        \n",
    "        ## Running RFE with i features\n",
    "        RF_rfe = RFE(estimator = RandomForestClassifier(n_estimators = 100, max_depth = 5), \n",
    "                     n_features_to_select = numb_features).fit(X_train, Y_train)\n",
    "        \n",
    "        ## Variables to be considered\n",
    "        to_select = X_train.columns[RF_rfe.support_]\n",
    "        to_select_list.append(RF_rfe.support_)\n",
    "        \n",
    "        ## Building the Random Forest model\n",
    "        X_train_md = X_train[to_select]\n",
    "        X_test_md = X_test[to_select]\n",
    "        \n",
    "        RF_md = RandomForestClassifier(n_estimators = 100, max_depth = 5).fit(X_train_md, Y_train)\n",
    "        \n",
    "        ## Predicting on the test data-frame and storing RMSE\n",
    "        results.append(log_loss(Y_test, RF_md.predict_proba(X_test_md)))\n",
    "\n",
    "    return results\n",
    "\n",
    "## Defining list to store results\n",
    "to_select_list = list()\n",
    "\n",
    "## Defining input and target variables\n",
    "X = training.drop(columns = ['interest_level']); Y = training['interest_level']\n",
    "\n",
    "## Running RFE to estimate number of features to be selected\n",
    "RFE_numb_features = RF_RFE_rep_cross_val(X, Y, numb_folds = 5, max_features = 31, numb_reps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800e5361-6d2f-4775-bb1f-2123cb6ccdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Identifying features\n",
    "features = pd.DataFrame(to_select_list)\n",
    "features.columns = X.columns\n",
    "feature_selections = 100 * features.apply(np.sum, axis = 0) / features.shape[0]\n",
    "feature_selections = pd.DataFrame(feature_selections).reset_index(drop = False)\n",
    "\n",
    "## Model performance given the number of variables\n",
    "feature_performance = pd.DataFrame(RFE_numb_features)\n",
    "feature_performance.columns = [['Split_1', 'Split_2', 'Split_3']]\n",
    "feature_performance['Mean'] = feature_performance.apply(np.mean, axis = 1)\n",
    "feature_performance['Num_features'] = feature_performance.index + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1881316-8c30-4a04-964d-ec25451ac5fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91209f3-9f9c-45fe-a5f4-dca18a90a7f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_selections.sort_values(0, ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75899570-f5bc-4486-ac88-479311362d1e",
   "metadata": {},
   "source": [
    "### 5. Hyper-Parameter Tuning\n",
    "Tuning the hyper-parameters for Random Forest, Hist Gradient Boosting, LightGBM, and XGBoost using Optuna framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a493a6-15e2-4237-ada0-8b691fa89864",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining optuna objective functions\n",
    "\n",
    "class rf_objective:\n",
    "\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(criterion = 'log_loss',\n",
    "                      n_estimators = trial.suggest_int('n_estimators', 100, 1500, step = 100),\n",
    "                      max_depth = trial.suggest_int('max_depth', 3, 12, step = 1),\n",
    "                      min_samples_split = trial.suggest_int('min_samples_split', 5, 100, step = 5),\n",
    "                      min_samples_leaf = trial.suggest_int('min_samples_leaf', 5, 100, step = 5))\n",
    "        scores = []\n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            Y_train , Y_valid = Y.iloc[train_idx] , Y.iloc[valid_idx]\n",
    "\n",
    "            model = RandomForestClassifier(**params).fit(X_train, Y_train)\n",
    "\n",
    "            preds_valid = model.predict_proba(X_valid)\n",
    "            scores.append(log_loss(Y_valid, preds_valid))\n",
    "        return np.mean(scores)\n",
    "                                   \n",
    "                                   \n",
    "class xgb_objective:\n",
    "\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(objective = 'multi:softprob',\n",
    "                      eval_metric = 'mlogloss',\n",
    "                      n_estimators = trial.suggest_int('n_estimators', 300, 1500, step = 100),\n",
    "                      learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                      max_depth = trial.suggest_int('max_depth', 3, 12, step = 1),\n",
    "                      gamma = trial.suggest_float('reg_alpha', 0, 100, step = 10),\n",
    "                      min_child_weight = trial.suggest_int('min_child_weight', 0, 200, step = 10),\n",
    "                      subsample = trial.suggest_float('subsample', 0.6, 1, step = 0.05), \n",
    "                      colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05))\n",
    "        scores = []\n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            Y_train , Y_valid = Y.iloc[train_idx] , Y.iloc[valid_idx]\n",
    "\n",
    "            model = XGBClassifier(**params).fit(X_train, Y_train)\n",
    "\n",
    "            preds_valid = model.predict_proba(X_valid)\n",
    "            scores.append(log_loss(Y_valid, preds_valid))\n",
    "        return np.mean(scores)\n",
    "                \n",
    "\n",
    "class lgbm_objective:\n",
    "\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(objective = 'multiclass',\n",
    "                      metric = 'multi_logloss',\n",
    "                      n_estimators = trial.suggest_int('n_estimators', 300, 1500, step = 100),\n",
    "                      learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                      max_depth = trial.suggest_int('max_depth', 3, 12, step = 1),\n",
    "                      reg_alpha = trial.suggest_float('reg_alpha', 0.1, 10, log = True),\n",
    "                      reg_lambda = trial.suggest_float('reg_lambda', 0.1, 10, log = True),\n",
    "                      num_leaves = trial.suggest_int('num_leaves', 11, 101, step = 5),\n",
    "                      subsample = trial.suggest_float('subsample', 0.4, 1, step = 0.05),\n",
    "                      colsample_bytree = trial.suggest_float('colsample_bytree', 0.6, 1, step = 0.05))\n",
    "        scores = []\n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            Y_train , Y_valid = Y.iloc[train_idx] , Y.iloc[valid_idx]\n",
    "\n",
    "            model = LGBMClassifier(**params).fit(X_train, Y_train)\n",
    "\n",
    "            preds_valid = model.predict_proba(X_valid)\n",
    "            scores.append(log_loss(Y_valid, preds_valid))\n",
    "        return np.mean(scores)\n",
    "                                   \n",
    "class hist_objective:\n",
    "\n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        \n",
    "        params = dict(max_iter = trial.suggest_int('max_iter', 300, 1000, step = 100),\n",
    "                      learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, step = 0.01),\n",
    "                      max_depth = trial.suggest_int('max_depth', 3, 12, step = 1),\n",
    "                      l2_regularization = trial.suggest_float('l2_regularization', 0.1, 10))\n",
    "        scores = []\n",
    "        skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = self.seed)\n",
    "        for train_idx, valid_idx in skf.split(X, Y):\n",
    "\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            Y_train , Y_valid = Y.iloc[train_idx] , Y.iloc[valid_idx]\n",
    "\n",
    "            model = HistGradientBoostingClassifier(**params).fit(X_train, Y_train)\n",
    "\n",
    "            preds_valid = model.predict_proba(X_valid)\n",
    "            scores.append(log_loss(Y_valid, preds_valid))\n",
    "        return np.mean(scores)\n",
    "    \n",
    "## Defining SEED and Trials\n",
    "SEED = 42\n",
    "N_TRIALS = 50\n",
    "\n",
    "## Defining input and target variables\n",
    "X = training.drop(columns = ['interest_level'])\n",
    "Y = training['interest_level']\n",
    "\n",
    "## Executing the optimization\n",
    "study_rf = optuna.create_study(direction = 'minimize')\n",
    "study_rf.optimize(rf_objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "study_xgb = optuna.create_study(direction = 'minimize')\n",
    "study_xgb.optimize(xgb_objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "study_lgbm = optuna.create_study(direction = 'minimize')\n",
    "study_lgbm.optimize(lgbm_objective(SEED), n_trials = N_TRIALS)\n",
    "\n",
    "study_hist = optuna.create_study(direction = 'minimize')\n",
    "study_hist.optimize(hist_objective(SEED), n_trials = N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840a8fed-c877-40a3-8448-6e32cfb22a9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(study_rf.best_trial.params)\n",
    "print(study_rf.best_trial.value)\n",
    "\n",
    "print(study_xgb.best_trial.params)\n",
    "print(study_xgb.best_trial.value)\n",
    "\n",
    "print(study_lgbm.best_trial.params)\n",
    "print(study_lgbm.best_trial.value)\n",
    "\n",
    "print(study_hist.best_trial.params)\n",
    "print(study_hist.best_trial.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0457f3a-6a1c-4935-88d8-3e9fdccae669",
   "metadata": {},
   "source": [
    "### 6. Modelling\n",
    "Building models with cross validation using optimized hyper-parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749111e7-60e8-4bf4-90e1-81c1c47d2cd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining input and target variables\n",
    "X = training.drop(columns = ['interest_level'])\n",
    "Y = training['interest_level']\n",
    "\n",
    "X_test = testing.drop(columns = ['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed4b46-31d5-4d45-8e6f-c797268baa13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## RandomForest:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = RandomForestClassifier(n_estimators = 700,\n",
    "                                   max_depth = 12,\n",
    "                                   min_samples_split = 5,\n",
    "                                   min_samples_leaf = 5).fit(X_train, Y_train)   \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "\n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "rf_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the RandomForest model over 10-folds is:', rf_cv_score)\n",
    "\n",
    "## Averaging RF model preds\n",
    "rf_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "rf_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = rf_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/rf_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77e8ec-5cbd-4fd3-b298-25f4751be9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## XGBoost:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = XGBClassifier(n_estimators = 1500,\n",
    "                          max_depth = 10,\n",
    "                          learning_rate = 0.17,\n",
    "                          gamma = 10, \n",
    "                          min_child_weight = 0, \n",
    "                          subsample = 0.85,\n",
    "                          colsample_bytree = 0.9, \n",
    "                          objective = 'multi:softprob', \n",
    "                          eval_metric = 'mlogloss').fit(X_train, Y_train)  \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "xgb_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the XGBoost model over 10-folds is:', xgb_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "xgb_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "xgb_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = xgb_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/xgb_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f0e02f-bd23-4a81-abfe-85b4f4ea6ae1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## LightGBM:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 4,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)    \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "lgbm_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the LightGBM model over 10-folds is:', lgbm_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "lgbm_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "lgbm_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = lgbm_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/lgbm_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ac837-f3fa-4097-a928-36796b0f5436",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## HistGradientBoosting:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "hist_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the HistGB model over 10-folds is:', hist_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "hist_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "hist_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = hist_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/hist_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab731db-a2bc-4a51-bd69-49281123de7d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ensembling\n",
    "Averaging predictions from different models using thier optimized hyper-parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1347bb-5612-4feb-b890-d0b4cffaac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)   \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94b8465-4d39-45a5-911b-42857d50b1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)   \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)\n",
    "    \n",
    "    model3 = XGBClassifier(n_estimators = 1500,\n",
    "                          max_depth = 10,\n",
    "                          learning_rate = 0.17,\n",
    "                          gamma = 10, \n",
    "                          min_child_weight = 0, \n",
    "                          subsample = 0.85,\n",
    "                          colsample_bytree = 0.9, \n",
    "                          objective = 'multi:softprob', \n",
    "                          eval_metric = 'mlogloss').fit(X_train, Y_train) \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    model3_pred_val = model3.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    model3_pred_test = model3.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val); val_preds.append(model3_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test); preds.append(model3_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens2_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e86047b-f00a-4e63-9f19-aff04c25a0e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 300,\n",
    "                                           max_depth = 12,\n",
    "                                           learning_rate = 0.02,\n",
    "                                           l2_regularization = 4.26).fit(X_train, Y_train)    \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 400,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.27,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 9.96, \n",
    "                           reg_lambda = 0.85, \n",
    "                           subsample = 0.85,\n",
    "                           colsample_bytree = 0.8).fit(X_train, Y_train) \n",
    "    \n",
    "    model4 = RandomForestClassifier(n_estimators = 800,\n",
    "                                   max_depth = 11,\n",
    "                                   min_samples_split = 50,\n",
    "                                   min_samples_leaf = 5).fit(X_train, Y_train)  \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    model4_pred_val = model4.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    model4_pred_test = model3.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val); val_preds.append(model4_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test); preds.append(model4_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens3_submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
