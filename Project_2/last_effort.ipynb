{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6809ae-1f42-4bef-a14d-10d71c415c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install lightgbm xgboost optuna tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8445714b-19c3-4c84-9ef0-207bc33ab671",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoding=utf8\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "import math\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier,ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,GradientBoostingRegressor,ExtraTreesRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.svm import LinearSVC,SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import log_loss,mean_absolute_error,mean_squared_error\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler,Normalizer,StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer,HashingVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB,GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "875154a5-4737-4438-9485-e0f5aa140c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def stacking(clf, train_x, train_y, test_x, clf_name, class_num = 3):\n",
    "    train = np.zeros((train_x.shape[0], class_num))\n",
    "    test = np.zeros((test_x.shape[0], class_num))\n",
    "    test_pre = np.empty((folds, test_x.shape[0], class_num))\n",
    "    cv_scores=[]\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        tr_x = train_x[train_index];tr_y = train_y[train_index]\n",
    "        te_x = train_x[test_index]; te_y = train_y[test_index]\n",
    "        \n",
    "        if clf_name in [\"rf\",\"ada\",\"gb\",\"et\",\"lr\",\"knn\",\"mnb\",\"ovr\",\"gnb\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre\n",
    "            test_pre[i,:] = clf.predict_proba(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"lsvc\"]:\n",
    "            clf.fit(tr_x,tr_y)\n",
    "            pre = clf.decision_function(te_x)\n",
    "            train[test_index] = pre\n",
    "            test_pre[i,:] = clf.decision_function(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"xgb\"]:\n",
    "            train_matrix = clf.DMatrix(tr_x, label = tr_y, missing = -1)\n",
    "            test_matrix = clf.DMatrix(te_x, label = te_y, missing = -1)\n",
    "            z = clf.DMatrix(test_x, label = te_y, missing = -1)\n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'multi:softprob',\n",
    "                      'eval_metric': 'mlogloss',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      \"num_class\": class_num}\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            watchlist = [(train_matrix, 'train'),\n",
    "                         (test_matrix, 'eval')\n",
    "                         ]\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_boost_round = num_round, evals = watchlist, \\\n",
    "                                  early_stopping_rounds = early_stopping_rounds)\n",
    "                pre = model.predict(test_matrix, ntree_limit = model.best_ntree_limit)\n",
    "                train[test_index] = pre\n",
    "                test_pre[i, :] = model.predict(z, ntree_limit = model.best_ntree_limit)\n",
    "                cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"lgb\"]:\n",
    "            train_matrix = clf.Dataset(tr_x, label = tr_y)\n",
    "            test_matrix = clf.Dataset(te_x, label = te_y)\n",
    "            #z = clf.Dataset(test_x, label=te_y)\n",
    "            #z=test_x\n",
    "            params = {\n",
    "                      'boosting_type': 'gbdt',\n",
    "                      'objective': 'multiclass',\n",
    "                      'metric': 'multi_logloss',\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'num_leaves': 2**5,\n",
    "                      'lambda_l2': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'learning_rate': 0.03,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2017,\n",
    "                      'nthread': 12,\n",
    "                      \"num_class\": class_num,\n",
    "                      'silent': True}\n",
    "            num_round = 10000\n",
    "            early_stopping_rounds = 100\n",
    "            if test_matrix:\n",
    "                model = clf.train(params, train_matrix, num_round,valid_sets = test_matrix, \\\n",
    "                                  early_stopping_rounds = early_stopping_rounds)\n",
    "                pre = model.predict(te_x, num_iteration = model.best_iteration)\n",
    "                train[test_index] = pre\n",
    "                test_pre[i, :] = model.predict(test_x, num_iteration = model.best_iteration)\n",
    "                cv_scores.append(log_loss(te_y, pre))\n",
    "        elif clf_name in [\"nn\"]:\n",
    "            from keras.layers import Dense, Dropout, BatchNormalization,SReLU\n",
    "            from keras.optimizers import SGD,RMSprop\n",
    "            from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "            from keras.utils import np_utils\n",
    "            from keras.regularizers import l2\n",
    "            from keras.models import Sequential\n",
    "            clf = Sequential()\n",
    "            clf.add(Dense(64, input_dim = tr_x.shape[1], activation = \"relu\", W_regularizer = l2()))\n",
    "            #clf.add(SReLU())\n",
    "            #clf.add(Dropout(0.2))\n",
    "            clf.add(Dense(64,activation = \"relu\", W_regularizer = l2()))\n",
    "            #clf.add(SReLU())\n",
    "            #clf.add(Dense(64, activation=\"relu\", W_regularizer=l2()))\n",
    "            # model.add(Dropout(0.2))\n",
    "            clf.add(Dense(class_num, activation = \"softmax\"))\n",
    "            clf.summary()\n",
    "            early_stopping = EarlyStopping(monitor = 'val_loss', patience = 20)\n",
    "            reduce = ReduceLROnPlateau(min_lr = 0.0002, factor = 0.05)\n",
    "            clf.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "            clf.fit(tr_x, tr_y, batch_size = 640, nb_epoch = 1000, validation_data = [te_x, te_y], callbacks = [early_stopping, reduce])\n",
    "            pre = clf.predict_proba(te_x)\n",
    "            train[test_index] = pre\n",
    "            test_pre[i,:] = clf.predict_proba(test_x)\n",
    "            cv_scores.append(log_loss(te_y, pre))\n",
    "        else:\n",
    "            raise IOError(\"Please add new clf.\")\n",
    "        print(\"%s now score is:\"%clf_name,cv_scores)\n",
    "        with open(\"score.txt\",\"a\") as f:\n",
    "            f.write(\"%s now score is:\"%clf_name+str(cv_scores)+\"\\n\")\n",
    "    test[:]=test_pre.mean(axis=0)\n",
    "    print(\"%s_score_list:\"%clf_name,cv_scores)\n",
    "    print(\"%s_score_mean:\"%clf_name,np.mean(cv_scores))\n",
    "    with open(\"score.txt\", \"a\") as f:\n",
    "        f.write(\"%s_score_mean:\"%clf_name+str(np.mean(cv_scores))+\"\\n\")\n",
    "    return train.reshape(-1,class_num),test.reshape(-1,class_num)\n",
    "\n",
    "def rf(x_train, y_train, x_valid):\n",
    "    randomforest = RandomForestClassifier(n_estimators=1200, max_depth=20, n_jobs=-1, random_state=2017, max_features=\"auto\",verbose=1)\n",
    "    rf_train, rf_test = stacking(randomforest, x_train, y_train, x_valid,\"rf\")\n",
    "    return rf_train, rf_test,\"rf\"\n",
    "\n",
    "def ada(x_train, y_train, x_valid):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking(adaboost, x_train, y_train, x_valid,\"ada\")\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb(x_train, y_train, x_valid):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking(gbdt, x_train, y_train, x_valid,\"gb\")\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et(x_train, y_train, x_valid):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking(extratree, x_train, y_train, x_valid,\"et\")\n",
    "    return et_train, et_test,\"et\"\n",
    "\n",
    "def ovr(x_train, y_train, x_valid):\n",
    "    est=RandomForestClassifier(n_estimators=400, max_depth=16, n_jobs=-1, random_state=2017, max_features=\"auto\",\n",
    "                               verbose=1)\n",
    "    ovr = OneVsRestClassifier(est,n_jobs=-1)\n",
    "    ovr_train, ovr_test = stacking(ovr, x_train, y_train, x_valid,\"ovr\")\n",
    "    return ovr_train, ovr_test,\"ovr\"\n",
    "\n",
    "def xgb(x_train, y_train, x_valid):\n",
    "    xgb_train, xgb_test = stacking(xgboost, x_train, y_train, x_valid,\"xgb\")\n",
    "    return xgb_train, xgb_test,\"xgb\"\n",
    "\n",
    "def lgb(x_train, y_train, x_valid):\n",
    "    xgb_train, xgb_test = stacking(lightgbm, x_train, y_train, x_valid,\"lgb\")\n",
    "    return xgb_train, xgb_test,\"lgb\"\n",
    "\n",
    "def gnb(x_train, y_train, x_valid):\n",
    "    gnb=GaussianNB()\n",
    "    gnb_train, gnb_test = stacking(gnb, x_train, y_train, x_valid,\"gnb\")\n",
    "    return gnb_train, gnb_test,\"gnb\"\n",
    "\n",
    "def lr(x_train, y_train, x_valid):\n",
    "    logisticregression=LogisticRegression(n_jobs=-1,random_state=2017,C=0.1,max_iter=200)\n",
    "    lr_train, lr_test = stacking(logisticregression, x_train, y_train, x_valid, \"lr\")\n",
    "    return lr_train, lr_test, \"lr\"\n",
    "\n",
    "def fm(x_train, y_train, x_valid):\n",
    "    pass\n",
    "\n",
    "\n",
    "def lsvc(x_train, y_train, x_valid):\n",
    "\n",
    "    #linearsvc=SVC(probability=True,kernel=\"linear\",random_state=2017,verbose=1)\n",
    "    #linearsvc=SVC(probability=True,kernel=\"linear\",random_state=2017,verbose=1)\n",
    "    linearsvc=LinearSVC(random_state=2017)\n",
    "    lsvc_train, lsvc_test = stacking(linearsvc, x_train, y_train, x_valid, \"lsvc\")\n",
    "    return lsvc_train, lsvc_test, \"lsvc\"\n",
    "\n",
    "def knn(x_train, y_train, x_valid):\n",
    "    #pca = PCA(n_components=10)\n",
    "    #pca.fit(x_train)\n",
    "    #x_train = pca.transform(x_train)\n",
    "    #x_valid = pca.transform(x_valid)\n",
    "\n",
    "    kneighbors=KNeighborsClassifier(n_neighbors=200,n_jobs=-1)\n",
    "    knn_train, knn_test = stacking(kneighbors, x_train, y_train, x_valid, \"knn\")\n",
    "    return knn_train, knn_test, \"knn\"\n",
    "\n",
    "def nn(x_train, y_train, x_valid):\n",
    "    from keras.utils import np_utils\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    nn_train, nn_test = stacking(\"\", x_train, y_train, x_valid, \"nn\")\n",
    "    return nn_train, nn_test, \"nn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddc3e727-e568-47e9-9fd5-fc2b9df89f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Reading the data\n",
    "train = pd.read_csv('Data/W23P2_train.csv')\n",
    "test = pd.read_csv('Data/W23P2_test.csv')\n",
    "sub = pd.read_csv('Data/Sample_Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "266cf6c1-8df5-4448-8187-de67a798ac5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Transforming interest_level label to numeric\n",
    "train['interest_level'] = np.where(train['interest_level'] == 'low', 0, \n",
    "                                   np.where(train['interest_level'] == 'medium', 1, 2))\n",
    "\n",
    "## Removing training observations with prices higher than $40,000\n",
    "train = train[train['price'] < 40000].reset_index(drop = True)\n",
    "\n",
    "## Removing outlier locations\n",
    "train = train[(train['latitude'] < 43) & (train['latitude'] > 1)].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba27e14a-e42a-40cf-a822-230d75741293",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24027/329181946.py:16: FutureWarning: In a future version of pandas all arguments of StringMethods.split except for the argument 'pat' will be keyword-only.\n",
      "  addies = pd.DataFrame(list(full_data['street_address'].str.split(' ', 1)), columns = ['number', 'street'])\n"
     ]
    }
   ],
   "source": [
    "## Combining data frames for cleaning and feature engineering purposes\n",
    "train.insert(0, 'train', 1)\n",
    "test.insert(0, 'train', 0)\n",
    "full_data = pd.concat([train.drop(columns = ['interest_level']), test.drop(columns = ['ID'])])\n",
    "\n",
    "## Making sure address variable is a string\n",
    "full_data['street_address'] = full_data['street_address'].astype(str)\n",
    "\n",
    "## Altering the missing values\n",
    "full_data['street_address'] = np.where(full_data['street_address'] == 'nan', '123 nan', full_data['street_address'])\n",
    "\n",
    "## Changing street address variable to all lowercase\n",
    "full_data['street_address'] = full_data['street_address'].str.lower() \n",
    "\n",
    "## Splitting street_address into number and street\n",
    "addies = pd.DataFrame(list(full_data['street_address'].str.split(' ', 1)), columns = ['number', 'street'])\n",
    "addies['street'] = ' ' + addies['street'] + ' '\n",
    "\n",
    "## Cleaning street variable\n",
    "addies['street'] = addies['street'].str.replace(' st ', ' street', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' st. ', ' street', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' blvd ', ' boulevard ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ave ', ' avenue ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ave. ', ' avenue ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' dr ', ' drive ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' pl ', ' place ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' e ', ' east ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' w ', ' west ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' first ', ' 1st ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' second ', ' 2nd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' third ', ' 3rd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' fourth ', ' 4th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' fifth ', ' 5th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' sixth ', ' 6th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' seventh ', ' 7th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' eighth ', ' 8th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace(' ninth ', ' 9th ', regex = True)\n",
    "\n",
    "addies['street'] = addies['street'].str.replace('1 ', '1st ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('2 ', '2nd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('3 ', '3rd ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('4 ', '4th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('5 ', '5th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('6 ', '6th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('7 ', '7th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('8 ', '8th ', regex = True)\n",
    "addies['street'] = addies['street'].str.replace('9 ', '9th ', regex = True)\n",
    "\n",
    "\n",
    "## Adding cleaned street_address variable back into the full_data data-frame\n",
    "full_data['street_address'] = addies['number'] + ' ' + addies['street']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb65e5b0-0a98-4b4f-96e8-2f9fb85cb9b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Adding geolocation features\n",
    "full_data['geo_area_50'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 50) % 50) * 50 + (int(-x[1] * 50) % 50), axis = 1)                                         \n",
    "                         \n",
    "full_data['geo_area_100'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 100) % 100) * 100 + (int(-x[1] * 100) % 100), axis = 1)                                         \n",
    "  \n",
    "full_data['geo_area_200'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:(int(x[0] * 200) % 200) * 200 + (int(-x[1] * 200) % 200), axis = 1)                                         \n",
    "\n",
    "## Financial district\n",
    "lat = 40.705628\n",
    "lon = -74.010278\n",
    "full_data['distance_to_fin'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:math.sqrt((x[0] - lat)**2 + (x[1] - lon)**2), axis = 1)\n",
    "\n",
    "## Central park\n",
    "lat = 40.785091\n",
    "lon = -73.968285\n",
    "full_data['distance_to_cp'] = full_data[['latitude', 'longitude']]\\\n",
    ".apply(lambda x:math.sqrt((x[0] - lat)**2 + (x[1] - lon)**2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "216939cd-2e7d-4bb9-a55b-6aa5dc2a7127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Log-transformation of the price variable\n",
    "full_data['log_price'] = np.log(full_data['price'])\n",
    "\n",
    "## Number of total rooms\n",
    "full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \n",
    "\n",
    "## Number of categorical features listed \n",
    "full_data['num_of_features'] = full_data.iloc[:, 7:67].sum(axis = 1)\n",
    "\n",
    "## Number of half-baths\n",
    "full_data['half_bathrooms'] = full_data['bathrooms'] - full_data['bathrooms'].apply(int)\n",
    "\n",
    "## Price per room\n",
    "full_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per bedroom\n",
    "full_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per bathroom\n",
    "full_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Price per feature listed\n",
    "full_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per room\n",
    "full_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per bedroom\n",
    "full_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)\n",
    "\n",
    "## Features per bathroom\n",
    "full_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]/x[1] if x[1]!=0 else 0, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f1fb04f-4039-4291-ac69-fb1cb5df8375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Making sure address variable is a string\n",
    "full_data['street_address'] = full_data['street_address'].astype(str)\n",
    "\n",
    "## Getting a count of observations with same address\n",
    "street = full_data['street_address'].value_counts()\n",
    "\n",
    "## Getting a count of observations with same number of bedrooms\n",
    "bedrooms = full_data['bedrooms'].value_counts()\n",
    "\n",
    "## Getting a count of observations with same number of bathrooms\n",
    "bathrooms = full_data['bathrooms'].value_counts()\n",
    "\n",
    "## Adding count information\n",
    "full_data['street_count'] = full_data['street_address'].apply(lambda x:street[x] if x == 'nan' else street[x])\n",
    "full_data['bedrooms_count'] = full_data['bedrooms'].apply(lambda x:bedrooms[x])\n",
    "full_data['bathrooms_count'] = full_data['bathrooms'].apply(lambda x:bathrooms[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5289de93-dadf-4f42-b30a-a231d33a33c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Computing density - number of listings in given area\n",
    "full_data['pos'] = full_data['longitude'].round(3).astype(str) + '_' + full_data['latitude'].round(3).astype(str)\n",
    "vals = full_data['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "full_data['density'] = full_data['pos'].apply(lambda x: dvals.get(x, vals.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a16889e5-cd82-40f6-bb30-045ad0be6765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Aggregating price variables by street address and computing summary statistics\n",
    "price_by_address = full_data.groupby('street_address')['price'].agg([np.min, np.max, np.median, np.mean]).reset_index()\n",
    "price_by_address.columns = ['street_address','min_price_by_address',\n",
    "                            'max_price_by_address','median_price_by_address','mean_price_by_address']\n",
    "\n",
    "## Adding aggregated price info\n",
    "full_data = pd.merge(full_data, price_by_address, how = 'left', on = 'street_address')\n",
    "\n",
    "## Computing percentile from aggregated price info\n",
    "full_data['price_percentile_by_address'] = full_data[['price','min_price_by_address','max_price_by_address']]\\\n",
    ".apply(lambda x:(x[0] - x[1]) / (x[2] - x[1]) if (x[2] - x[1]) != 0 else 0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bccd3ef-afb2-4256-bb76-4ba4966d7325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Aggregating price variables by position and computing summary statistics\n",
    "price_by_pos = full_data.groupby('pos')['price'].agg([np.min, np.max, np.median, np.mean]).reset_index()\n",
    "price_by_pos.columns = ['pos','min_price_by_pos',\n",
    "                            'max_price_by_pos','median_price_by_pos','mean_price_by_pos']\n",
    "\n",
    "## Adding aggregated price info\n",
    "full_data = pd.merge(full_data, price_by_pos, how = 'left', on = 'pos')\n",
    "\n",
    "## Computing percentile from aggregated price info\n",
    "full_data['price_percentile_by_pos'] = full_data[['price','min_price_by_pos','max_price_by_pos']]\\\n",
    ".apply(lambda x:(x[0] - x[1]) / (x[2] - x[1]) if (x[2] - x[1]) != 0 else 0.5, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2c8e552-9ec1-4fa2-b8d3-e470cd9f3733",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Computing market price for combination of address, bedrooms, and bathrooms\n",
    "mkt_price = full_data.groupby(['street_address', 'bedrooms', 'bathrooms']).price.mean().reset_index()\n",
    "\n",
    "## Adding market price information\n",
    "mkt_price = pd.merge(full_data[['street_address', 'bedrooms', 'bathrooms']], mkt_price, how = 'left', \n",
    "                     on = ['street_address', 'bedrooms', 'bathrooms']).price\n",
    "full_data['mkt_price'] = mkt_price.values\n",
    "\n",
    "## Computing the difference to market price\n",
    "full_data['diff_to_mkt_price'] = full_data['price'] - full_data['mkt_price']\n",
    "\n",
    "## Computing the ratio to market price\n",
    "full_data['ratio_to_mkt_price'] = full_data['price'] / full_data['mkt_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54bc7448-9d39-48d1-8abd-d42529b33ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Condensing redundant variables after feature engineering\n",
    "\n",
    "laundry_vars = ['Laundry.in.Building', 'Laundry.in.Unit', 'Laundry.In.Building', 'Laundry.In.Unit', 'LAUNDRY', 'Washer.in.Unit', \n",
    "                'Dryer.in.Unit', 'Laundry.Room', 'Laundry', 'On.site.laundry', 'On.site.Laundry', 'Washer.Dryer', 'Washer.Dryer.in.building', \n",
    "                'In.Unit.Washer.Dryer', 'Washer...Dryer', 'Washer.Dryer.in.Unit']\n",
    "parking_vars = ['Parking.Space', 'Garage', 'Parking', 'On.site.Garage', 'assigned.parking.space', 'Common.parking.Garage', 'Full.Service.Garage', \n",
    "               'On.site.Parking.Lot', 'Private.parking']\n",
    "valet_vars = ['Valet.Parking', 'Valet']\n",
    "deck_vars = ['Roof.Deck', 'Balcony', 'Terrace', 'Patio', 'Roof.deck', 'balcony', 'terrace', 'patio', 'private.balcony', 'Private.balcony', \n",
    "             'Private.Deck', 'Common.roof.deck', 'ROOFDECK']\n",
    "outdoor_vars = ['Courtyard', 'Outdoor.Entertainment.Space', 'Private.Outdoor.Space', 'private.outdoor.space', 'Private.outdoor.space', \n",
    "                'Common.Outdoor.Space', 'PublicOutdoor', 'Outdoor.Space', 'Outdoor.Areas', 'Common.backyard', 'building.common.outdoor.space']\n",
    "garden_vars = ['Common.garden', 'garden', 'Garden.Patio', 'Garden', 'Residents.Garden']\n",
    "dishwasher_vars = ['Dishwasher', 'dishwasher']\n",
    "gym_vars = ['Fitness.Center', 'Gym.Fitness', 'Health.Club', 'Gym', 'gym', 'Gym.In.Building']\n",
    "pool_vars = ['Swimming.Pool', 'Pool', 'pool', 'Indoor.Pool']\n",
    "elevator_vars = ['Elevator', 'elevator']\n",
    "storage_vars = ['Storage', 'storage', 'Basement.Storage']\n",
    "internet_vars = ['High.Speed.Internet', 'WiFi', 'WiFi.Access']\n",
    "bike_vars = ['Bike.room', 'Bike.Room']\n",
    "pet_friendly_vars = ['Dogs.Allowed', 'Cats.Allowed', 'Pet.Friendly', 'Pets.on.approval']\n",
    "concierge_vars = ['Concierge', 'Concierge.Service', 'X24.7.Concierge']\n",
    "doorman_vars = ['Doorman', 'Full.time.doorman', 'Virtual.Doorman', 'FT.Doorman', 'doorman']\n",
    "super_vars = ['LIVE.IN.SUPER', 'Live.in.superintendent', 'Live.In.Superintendent','Live.in.Super', 'Live.In.Super']\n",
    "hardwood_vars = ['Hardwood.Floors', 'HARDWOOD', 'Hardwood.floors', 'Hardwood']\n",
    "ceiling_vars = ['High.ceilings', 'High.Ceilings', 'HIGH.CEILINGS', 'High.Ceiling']\n",
    "brick_vars = ['EXPOSED.BRICK', 'Exposed.Brick']\n",
    "construction_vars = ['New.Construction', 'Newly.renovated','Renovated', 'renovated', 'New.construction']\n",
    "photo_vars = ['Actual.Apt..Photos', 'ACTUAL.APT..PHOTOS']\n",
    "lounge_vars = ['Residents.Lounge', 'Lounge.room', 'Lounge']\n",
    "playroom_vars = ['Childrens.Playroom', 'Children.s.Playroom']\n",
    "ac_vars = ['Central.A.C', 'Air.conditioning']\n",
    "kitchen_vars = ['EAT.IN.KITCHEN','Eat.In.Kitchen']\n",
    "no_fee_vars = ['No.Fee', 'NO.FEE']\n",
    "accessibity_vars = ['Wheelchair.Ramp', 'Wheelchair.Access']\n",
    "multi_level_vars = ['Multi.Level', 'Multi.level']\n",
    "fireplace_vars = ['Fireplace', 'Decorative.Fireplace']\n",
    "highrise_vars = ['Hi.Rise', 'HIGHRISE']\n",
    "marble_bath_vars = ['Marble.Bath', 'Marble.Bathroom']\n",
    "prewar_vars = ['Pre.War', 'prewar', 'Prewar']\n",
    "\n",
    "def condense(data):\n",
    "    \n",
    "    ## Condensing data\n",
    "    data['Has_Laundry'] = np.where(np.sum(data[laundry_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Parking'] = np.where(np.sum(data[parking_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Valet'] = np.where(np.sum(data[valet_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Deck'] = np.where(np.sum(data[deck_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Outdoor_Area'] = np.where(np.sum(data[outdoor_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Garden'] = np.where(np.sum(data[garden_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Dishwasher'] = np.where(np.sum(data[dishwasher_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Gym'] = np.where(np.sum(data[gym_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Pool'] = np.where(np.sum(data[pool_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Elevator'] = np.where(np.sum(data[elevator_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Storage'] = np.where(np.sum(data[storage_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Wifi'] = np.where(np.sum(data[internet_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Bike'] = np.where(np.sum(data[bike_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Pet_Friendly'] = np.where(np.sum(data[pet_friendly_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Concierge'] = np.where(np.sum(data[concierge_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Doorman'] = np.where(np.sum(data[doorman_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Super'] = np.where(np.sum(data[super_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Hardwood_Floor'] = np.where(np.sum(data[hardwood_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_High_Ceilings'] = np.where(np.sum(data[ceiling_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Brick'] = np.where(np.sum(data[brick_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Renovated'] = np.where(np.sum(data[construction_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Photos'] = np.where(np.sum(data[photo_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Lounge'] = np.where(np.sum(data[lounge_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Playroom'] = np.where(np.sum(data[playroom_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_AC'] = np.where(np.sum(data[ac_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Has_Kitchen'] = np.where(np.sum(data[kitchen_vars], axis = 1) > 0, 1, 0)\n",
    "    data['No_Fee'] = np.where(np.sum(data[no_fee_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Accessible'] = np.where(np.sum(data[accessibity_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Multi_Level'] = np.where(np.sum(data[multi_level_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Fire'] = np.where(np.sum(data[fireplace_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Highrise'] = np.where(np.sum(data[highrise_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Marble_Bath'] = np.where(np.sum(data[marble_bath_vars], axis = 1) > 0, 1, 0)\n",
    "    data['Pre_War'] = np.where(np.sum(data[prewar_vars], axis = 1) > 0, 1, 0)\n",
    "    return data\n",
    "    \n",
    "## Applying function to full_data\n",
    "full_data = condense(full_data)\n",
    "\n",
    "## Dropping unnecessary columns\n",
    "to_drop = [laundry_vars, parking_vars, valet_vars, deck_vars, outdoor_vars,garden_vars, dishwasher_vars, gym_vars, pool_vars, elevator_vars, \n",
    "           storage_vars, internet_vars, bike_vars, pet_friendly_vars, concierge_vars, doorman_vars, super_vars, hardwood_vars, ceiling_vars, \n",
    "           brick_vars, construction_vars, photo_vars, lounge_vars, playroom_vars, ac_vars, kitchen_vars, no_fee_vars, accessibity_vars, \n",
    "           multi_level_vars, fireplace_vars, highrise_vars, marble_bath_vars, prewar_vars] \n",
    "\n",
    "for cols in to_drop:\n",
    "    full_data = full_data.drop(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333ca394-d588-4bcc-b693-98b24d565ba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Label encoding categorical variables\n",
    "cat_vars = ['Dining.Room', 'Reduced.Fee', 'Exclusive', 'No.pets', 'LOWRISE', 'SIMPLEX', 'Furnished', 'Loft', 'Stainless.Steel.Appliances', 'View', \n",
    "            'Green.Building', 'Short.Term.Allowed', 'Subway', 'Granite.Kitchen', 'Light', 'Guarantors.Accepted', 'Skylight', 'Sauna', 'Live.Work', \n",
    "            'Duplex', 'Walk.in.Closet.s.', 'Luxury.building', 'Post.War', 'Cable.Satellite.TV', 'Microwave', 'Sublet', 'Shares.OK', 'Has_Laundry', \n",
    "            'Has_Parking', 'Has_Valet', 'Has_Deck', 'Has_Outdoor_Area', 'Has_Garden', 'Has_Dishwasher', 'Has_Gym', 'Has_Pool', 'Has_Elevator', \n",
    "            'Has_Storage', 'Has_Wifi', 'Has_Bike', 'Pet_Friendly', 'Has_Concierge', 'Has_Doorman', 'Has_Super', 'Has_Hardwood_Floor', \n",
    "            'Has_High_Ceilings', 'Has_Brick', 'Has_Renovated', 'Has_Photos', 'Has_Lounge', 'Has_Playroom', 'Has_AC', 'Has_Kitchen', 'No_Fee', \n",
    "            'Accessible', 'Multi_Level', 'Fire', 'Highrise', 'Marble_Bath', 'Pre_War']\n",
    "LE_vars = []\n",
    "for cat_var in cat_vars:\n",
    "    LE_var = cat_var\n",
    "    full_data[LE_var] = LabelEncoder().fit_transform(full_data[cat_var])\n",
    "    LE_vars.append(LE_var)\n",
    "    \n",
    "## OneHot Encoding all catategorical variables\n",
    "oh_encoder = OneHotEncoder(sparse_output = True).fit(full_data[LE_vars])\n",
    "oh_sparse = oh_encoder.transform(full_data[LE_vars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b425d593-aa67-442d-80bb-3c344e700963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Splitting full_data back into training and testing sets\n",
    "training = full_data[full_data['train'] == 1].drop(columns = ['train', 'street_address', 'pos']).reset_index(drop = True)\n",
    "training['interest_level'] = train['interest_level']\n",
    "\n",
    "testing = full_data[full_data['train'] == 0].drop(columns = ['train', 'street_address', 'pos']).reset_index(drop = True)\n",
    "testing['ID'] = test['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90139b9a-41e4-4c87-a128-bf954c04d5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312fa80-e6eb-4db1-bf83-871e2a846cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a446ad2c-feed-4d08-813b-c07b5e8a7363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b3ac88-2694-4706-80cf-17738f7599b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd05b231-21a8-45bd-9aa2-f6db35aec617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "079130a9-3893-4515-9818-214bca9e1419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining input and target variables\n",
    "X = training.drop(columns = ['interest_level'])\n",
    "Y = training['interest_level']\n",
    "\n",
    "X_test = testing.drop(columns = ['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9f3cad78-afb9-4e16-b21e-3844166416c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 : log-loss-score ==> 0.8642745903808168\n",
      "Fold 1 : log-loss-score ==> 0.8775092599855586\n",
      "Fold 2 : log-loss-score ==> 0.8760201258324608\n",
      "Fold 3 : log-loss-score ==> 0.8487048246308411\n",
      "Fold 4 : log-loss-score ==> 0.8786928588201087\n",
      "Fold 5 : log-loss-score ==> 0.8427746524002978\n",
      "Fold 6 : log-loss-score ==> 0.8844626117665872\n",
      "Fold 7 : log-loss-score ==> 0.8643928533641023\n",
      "Fold 8 : log-loss-score ==> 0.878234290227424\n",
      "Fold 9 : log-loss-score ==> 0.8426762076080894\n",
      "Average log-loss of the RandomForest model over 10-folds is: 0.8657742275016288\n"
     ]
    }
   ],
   "source": [
    "## RandomForest:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = RandomForestClassifier(n_estimators = 700,\n",
    "                                   max_depth = 12,\n",
    "                                   min_samples_split = 5,\n",
    "                                   min_samples_leaf = 5).fit(X_train, Y_train)   \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "\n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "rf_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the RandomForest model over 10-folds is:', rf_cv_score)\n",
    "\n",
    "## Averaging RF model preds\n",
    "rf_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "rf_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = rf_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/rf_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e6eba1-35fc-4333-84b8-c4558cb19546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## XGBoost:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = XGBClassifier(n_estimators = 1500,\n",
    "                          max_depth = 10,\n",
    "                          learning_rate = 0.17,\n",
    "                          gamma = 10, \n",
    "                          min_child_weight = 0, \n",
    "                          subsample = 0.85,\n",
    "                          colsample_bytree = 0.9, \n",
    "                          objective = 'multi:softprob', \n",
    "                          eval_metric = 'mlogloss').fit(X_train, Y_train)  \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "xgb_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the XGBoost model over 10-folds is:', xgb_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "xgb_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "xgb_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = xgb_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/xgb_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "453f5652-0f81-4eba-9452-f39cac6fc4af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 : log-loss-score ==> 0.8203649100532943\n",
      "Fold 1 : log-loss-score ==> 0.8628034085206576\n",
      "Fold 2 : log-loss-score ==> 0.8407055398653133\n",
      "Fold 3 : log-loss-score ==> 0.808114477807324\n",
      "Fold 4 : log-loss-score ==> 0.8478417964727517\n",
      "Fold 5 : log-loss-score ==> 0.7899799375606462\n",
      "Fold 6 : log-loss-score ==> 0.8497054161714602\n",
      "Fold 7 : log-loss-score ==> 0.839633937661767\n",
      "Fold 8 : log-loss-score ==> 0.8371491948760821\n",
      "Fold 9 : log-loss-score ==> 0.8162476891534527\n",
      "Average log-loss of the LightGBM model over 10-folds is: 0.831254630814275\n"
     ]
    }
   ],
   "source": [
    "## LightGBM:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 4,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)    \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "lgbm_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the LightGBM model over 10-folds is:', lgbm_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "lgbm_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "lgbm_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = lgbm_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/lgbm_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6476cdd0-e57f-4239-a25c-596e95958f73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 : log-loss-score ==> 0.8253009798296974\n",
      "Fold 1 : log-loss-score ==> 0.8414092444773742\n",
      "Fold 2 : log-loss-score ==> 0.8289861335771727\n",
      "Fold 3 : log-loss-score ==> 0.8034629816313835\n",
      "Fold 4 : log-loss-score ==> 0.8301829335018452\n",
      "Fold 5 : log-loss-score ==> 0.782276653693204\n",
      "Fold 6 : log-loss-score ==> 0.8341674063484721\n",
      "Fold 7 : log-loss-score ==> 0.8230944877909459\n",
      "Fold 8 : log-loss-score ==> 0.826377898875996\n",
      "Fold 9 : log-loss-score ==> 0.8065535897179091\n",
      "Average log-loss of the HistGB model over 10-folds is: 0.8201812309444\n"
     ]
    }
   ],
   "source": [
    "## HistGradientBoosting:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "hist_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the HistGB model over 10-folds is:', hist_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "hist_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "hist_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = hist_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/hist_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83074947-8ab5-4a0d-93b4-b31212fca7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## AdaBoost:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = AdaBoostClassifier(n_estimators=500, \n",
    "                               random_state=2017, \n",
    "                               learning_rate=0.01).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ada_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the AdaBoost model over 10-folds is:', ada_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ada_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ada_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ada_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ada_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac0785-a8ef-4a6f-ac10-d553b72b1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GradientBoosting:\n",
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model = GradientBoostingClassifier(learning_rate=0.04, \n",
    "                                       n_estimators=100, \n",
    "                                       subsample=0.8, \n",
    "                                       random_state=2017,\n",
    "                                       max_depth=5,\n",
    "                                       verbose=1).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model_pred_val = model.predict_proba(X_val)\n",
    "    model_pred_test = model.predict_proba(X_test)\n",
    "        \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, model_pred_val)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "gbm_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the GBM model over 10-folds is:', gbm_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "gbm_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "gbm_preds_test.columns = model.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = gbm_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/gbm_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff11de-368e-443d-a64b-3996245e5da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60310fc-31a2-40e3-b333-767c023b3fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ada(x_train, y_train, x_valid):\n",
    "    adaboost = AdaBoostClassifier(n_estimators=50, random_state=2017, learning_rate=0.01)\n",
    "    ada_train, ada_test = stacking(adaboost, x_train, y_train, x_valid,\"ada\")\n",
    "    return ada_train, ada_test,\"ada\"\n",
    "\n",
    "def gb(x_train, y_train, x_valid):\n",
    "    gbdt = GradientBoostingClassifier(learning_rate=0.04, n_estimators=100, subsample=0.8, random_state=2017,max_depth=5,verbose=1)\n",
    "    gbdt_train, gbdt_test = stacking(gbdt, x_train, y_train, x_valid,\"gb\")\n",
    "    return gbdt_train, gbdt_test,\"gb\"\n",
    "\n",
    "def et(x_train, y_train, x_valid):\n",
    "    extratree = ExtraTreesClassifier(n_estimators=1200, max_depth=35, max_features=\"auto\", n_jobs=-1, random_state=2017,verbose=1)\n",
    "    et_train, et_test = stacking(extratree, x_train, y_train, x_valid,\"et\")\n",
    "    return et_train, et_test,\"et\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c3d99-44a6-461c-9894-f8e2823cf8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959b60c8-5994-473c-86a8-b3012fdc0c21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e2b6d8-610c-4f60-9e39-d858d116362f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ac204ca-8ef7-4dd2-a400-321da9b51113",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ensembling\n",
    "Averaging predictions from different models using thier optimized hyper-parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58a5756-9afa-464b-b6a7-82e918804292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)   \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)\n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba492c9-a158-48a1-a260-ac2c7dd39804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 500,\n",
    "                                           max_depth = 9,\n",
    "                                           learning_rate = 0.01,\n",
    "                                           l2_regularization = 4.88).fit(X_train, Y_train)   \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 500,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.06,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 2.41, \n",
    "                           reg_lambda = 0.15, \n",
    "                           subsample = 0.95,\n",
    "                           colsample_bytree = 0.6).fit(X_train, Y_train)\n",
    "    \n",
    "    model3 = XGBClassifier(n_estimators = 1500,\n",
    "                          max_depth = 10,\n",
    "                          learning_rate = 0.17,\n",
    "                          gamma = 10, \n",
    "                          min_child_weight = 0, \n",
    "                          subsample = 0.85,\n",
    "                          colsample_bytree = 0.9, \n",
    "                          objective = 'multi:softprob', \n",
    "                          eval_metric = 'mlogloss').fit(X_train, Y_train) \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    model3_pred_val = model3.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    model3_pred_test = model3.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val); val_preds.append(model3_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test); preds.append(model3_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens2_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954f6a8c-54ed-4493-ab91-f4429b105c20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cv_scores, log_loss_scores = list(), list()\n",
    "preds = list()\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 10, random_state = 42, shuffle = True)\n",
    "for i, (train_idx, test_idx) in enumerate(skf.split(X, Y)):\n",
    "\n",
    "    ## Splitting the data \n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    Y_train, Y_val = Y.iloc[train_idx], Y.iloc[test_idx]\n",
    "    \n",
    "    ## Initializing val_preds list\n",
    "    val_preds = list()\n",
    "                \n",
    "    ## Building LightGBM model\n",
    "    model1 = HistGradientBoostingClassifier(max_iter = 300,\n",
    "                                           max_depth = 12,\n",
    "                                           learning_rate = 0.02,\n",
    "                                           l2_regularization = 4.26).fit(X_train, Y_train)    \n",
    "    \n",
    "    model2 = LGBMClassifier(n_estimators = 400,\n",
    "                           max_depth = 3,\n",
    "                           learning_rate = 0.27,\n",
    "                           num_leaves = 56,\n",
    "                           reg_alpha = 9.96, \n",
    "                           reg_lambda = 0.85, \n",
    "                           subsample = 0.85,\n",
    "                           colsample_bytree = 0.8).fit(X_train, Y_train) \n",
    "    \n",
    "    model4 = RandomForestClassifier(n_estimators = 800,\n",
    "                                   max_depth = 11,\n",
    "                                   min_samples_split = 50,\n",
    "                                   min_samples_leaf = 5).fit(X_train, Y_train)  \n",
    "    \n",
    "    ## Predicting on X_val and X_test\n",
    "    model1_pred_val = model1.predict_proba(X_val)\n",
    "    model2_pred_val = model2.predict_proba(X_val)\n",
    "    #model3_pred_val = model3.predict_proba(X_val)\n",
    "    model4_pred_val = model4.predict_proba(X_val)\n",
    "    \n",
    "    model1_pred_test = model1.predict_proba(X_test)\n",
    "    model2_pred_test = model2.predict_proba(X_test)\n",
    "    #model3_pred_test = model3.predict_proba(X_test)\n",
    "    model4_pred_test = model3.predict_proba(X_test)\n",
    "    \n",
    "    ## Averaging val predictions\n",
    "    val_preds.append(model1_pred_val); val_preds.append(model2_pred_val); val_preds.append(model4_pred_val)\n",
    "    val_preds = pd.DataFrame(np.mean(val_preds, axis = 0))\n",
    "    \n",
    "    ## Computing log-loss\n",
    "    score = log_loss(Y_val, val_preds)\n",
    "    log_loss_scores.append(score)\n",
    "    preds.append(model1_pred_test); preds.append(model2_pred_test); preds.append(model4_pred_test)\n",
    "    print('Fold', i, ': log-loss-score ==>', score)\n",
    "        \n",
    "## Appending average cv scores\n",
    "cv_scores.append(np.mean(log_loss_scores))\n",
    "ens_cv_score = np.mean(cv_scores)    \n",
    "print('Average log-loss of the Ensemble model over 10-folds is:', ens_cv_score)\n",
    "\n",
    "## Averaging LGBM model preds\n",
    "ens_preds_test = pd.DataFrame(np.mean(preds, axis = 0))\n",
    "\n",
    "## Renaming columns by label class\n",
    "ens_preds_test.columns = model1.classes_\n",
    "\n",
    "## Creating submission file\n",
    "sub[['high', 'medium', 'low']] = ens_preds_test[[2, 1, 0]]\n",
    "sub.to_csv('Submissions/ens3_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2de59-7de8-4aaa-87c5-5bd2207eaa0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f29dbb4-d986-47ce-85f3-8e5c5c49f0fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2dff69-44a8-4bce-9dee-bb6b838dc27e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
